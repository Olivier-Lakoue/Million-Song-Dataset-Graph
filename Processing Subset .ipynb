{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Model of the Million Song Dataset\n",
    "\n",
    "*Andrea Soto*  \n",
    "*MIDS W205 Final Project*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Subset Data - 10,000 songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a project directory\n",
    "!mkdir msd_project\n",
    "!cd msd_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing download_subsetdata.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile download_subsetdata.sh\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "#Create a directory for the data\n",
    "mkdir data\n",
    "cd data\n",
    "\n",
    "# Download data subset of 10,000 songs, ~1GB to develop and test code\n",
    "wget http://static.echonest.com/millionsongsubset_full.tar.gz data_subset\n",
    "wait\n",
    "\n",
    "tar xvzf millionsongsubset_full.tar.gz\n",
    "wait\n",
    "\n",
    "# Download list of all artist ID \n",
    "# The format is: artist id<SEP>artist mbid<SEP>track id<SEP>artist name\n",
    "wget http://labrosa.ee.columbia.edu/millionsong/sites/default/files/AdditionalFiles/unique_artists.txt\n",
    "wait\n",
    "wc -l unique_artists.txt #44745 unique_artists.txt\n",
    "\n",
    "# Download list of all unique artist terms (Echo Nest tags) \n",
    "wget http://labrosa.ee.columbia.edu/millionsong/sites/default/files/AdditionalFiles/unique_terms.txt\n",
    "wait\n",
    "wc -l unique_terms.txt #7643 unique_terms.txt\n",
    "    \n",
    "# Download list of all unique artist musicbrainz tags\n",
    "wget http://labrosa.ee.columbia.edu/millionsong/sites/default/files/AdditionalFiles/unique_mbtags.txt\n",
    "wait\n",
    "wc -l unique_mbtags.txt #2321 unique_mbtags.txt\n",
    "\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Million Song Dataset is stored in HDF5 files. The data was transformed into csv files which can then be used in Neo4j to create the nodes and relationships of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List the files and store them in a separate .txt file\n",
    "import glob\n",
    "\n",
    "song_files = glob.glob('./data/MillionSongSubset/data/*/*/*/*.h5')\n",
    "list_file = './data/list_files.txt'\n",
    "\n",
    "with open(list_file,'w') as f:\n",
    "    f.writelines('\\n'.join(p for p in song_files))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Spark\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "#Escape L for line numbers\n",
    "spark_home = os.environ['SPARK_HOME'] = '/data/spark15'\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "execfile(os.path.join(spark_home,'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_paths = sc.textFile('file:///data/asoto/projectW205/data/list_files.txt')\n",
    "#file_paths = sc.textFile('/data/asoto/projectW205/data/list_files.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_paths.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def read_h5_file(path):\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        return f['metadata']['songs']['title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "songs = file_paths.map(read_h5_file)\n",
    "songs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_code/count_h5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_code/count_h5.py\n",
    "#!/usr/bin/env python\n",
    "from pyspark import SparkContext\n",
    "import time\n",
    "import h5py\n",
    "\n",
    "def read_h5_file(path):\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        return f['metadata']['songs']['title'][0]\n",
    "#Start Time\n",
    "t1 = time.time()\n",
    "\n",
    "# --- Process files ----\n",
    "sc = SparkContext(appName=\"SparkHDF5\")\n",
    "file_paths = sc.textFile('file:///data/asoto/projectW205/data/list_files.txt')\n",
    "\n",
    "songs = file_paths.map(read_h5_file)\n",
    "songs.count()\n",
    "# ----------------------\n",
    "\n",
    "#End Time\n",
    "t2 = time.time()\n",
    "sec = t2-t1\n",
    "\n",
    "print \"Time: %0.2f sec = %.2f min = %.2f h\"%(sec,sec/60.0,sec/1440.0)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/29 07:08:53 INFO spark.SparkContext: Running Spark version 1.3.0\n",
      "15/11/29 07:08:53 INFO spark.SecurityManager: Changing view acls to: asoto\n",
      "15/11/29 07:08:53 INFO spark.SecurityManager: Changing modify acls to: asoto\n",
      "15/11/29 07:08:53 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(asoto); users with modify permissions: Set(asoto)\n",
      "15/11/29 07:08:54 INFO slf4j.Slf4jLogger: Slf4jLogger started\n",
      "15/11/29 07:08:54 INFO Remoting: Starting remoting\n",
      "15/11/29 07:08:54 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@ip-10-149-10-206.ec2.internal:60992]\n",
      "15/11/29 07:08:54 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriver@ip-10-149-10-206.ec2.internal:60992]\n",
      "15/11/29 07:08:54 INFO util.Utils: Successfully started service 'sparkDriver' on port 60992.\n",
      "15/11/29 07:08:54 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "15/11/29 07:08:54 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "15/11/29 07:08:54 INFO storage.DiskBlockManager: Created local directory at /tmp/spark-badb9c72-0cf3-469b-bac0-9b737bc8a64e/blockmgr-8ff7ef2f-2c33-49a2-94fd-3036598da18f\n",
      "15/11/29 07:08:54 INFO storage.MemoryStore: MemoryStore started with capacity 265.4 MB\n",
      "15/11/29 07:08:54 INFO spark.HttpFileServer: HTTP File server directory is /tmp/spark-b67ff058-6aad-4558-92c3-04abfbd32603/httpd-d0653ea5-039b-4568-86b6-7e75736c72ee\n",
      "15/11/29 07:08:54 INFO spark.HttpServer: Starting HTTP Server\n",
      "15/11/29 07:08:54 INFO server.Server: jetty-8.y.z-SNAPSHOT\n",
      "15/11/29 07:08:54 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:54170\n",
      "15/11/29 07:08:54 INFO util.Utils: Successfully started service 'HTTP file server' on port 54170.\n",
      "15/11/29 07:08:54 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "15/11/29 07:08:54 INFO server.Server: jetty-8.y.z-SNAPSHOT\n",
      "15/11/29 07:08:54 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040\n",
      "15/11/29 07:08:54 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "15/11/29 07:08:54 INFO ui.SparkUI: Started SparkUI at http://ip-10-149-10-206.ec2.internal:4040\n",
      "15/11/29 07:08:54 INFO util.Utils: Copying /data/asoto/projectW205/test_code/count_h5.py to /tmp/spark-51519a1b-a3bb-4cab-8661-87ddb6129223/userFiles-61316a33-874d-43e9-b8cc-0fd3e27f7cff/count_h5.py\n",
      "15/11/29 07:08:54 INFO spark.SparkContext: Added file file:/data/asoto/projectW205/test_code/count_h5.py at file:/data/asoto/projectW205/test_code/count_h5.py with timestamp 1448780934952\n",
      "15/11/29 07:08:55 INFO executor.Executor: Starting executor ID <driver> on host localhost\n",
      "15/11/29 07:08:55 INFO util.AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@ip-10-149-10-206.ec2.internal:60992/user/HeartbeatReceiver\n",
      "15/11/29 07:08:55 INFO netty.NettyBlockTransferService: Server created on 59005\n",
      "15/11/29 07:08:55 INFO storage.BlockManagerMaster: Trying to register BlockManager\n",
      "15/11/29 07:08:55 INFO storage.BlockManagerMasterActor: Registering block manager localhost:59005 with 265.4 MB RAM, BlockManagerId(<driver>, localhost, 59005)\n",
      "15/11/29 07:08:55 INFO storage.BlockManagerMaster: Registered BlockManager\n",
      "15/11/29 07:08:55 INFO storage.MemoryStore: ensureFreeSpace(277011) called with curMem=0, maxMem=278302556\n",
      "15/11/29 07:08:55 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 270.5 KB, free 265.1 MB)\n",
      "15/11/29 07:08:55 INFO storage.MemoryStore: ensureFreeSpace(21084) called with curMem=277011, maxMem=278302556\n",
      "15/11/29 07:08:55 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 265.1 MB)\n",
      "15/11/29 07:08:55 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:59005 (size: 20.6 KB, free: 265.4 MB)\n",
      "15/11/29 07:08:55 INFO storage.BlockManagerMaster: Updated info of block broadcast_0_piece0\n",
      "15/11/29 07:08:55 INFO spark.SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2\n",
      "15/11/29 07:08:56 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/11/29 07:08:56 INFO spark.SparkContext: Starting job: count at /data/asoto/projectW205/test_code/count_h5.py:15\n",
      "15/11/29 07:08:56 INFO scheduler.DAGScheduler: Got job 0 (count at /data/asoto/projectW205/test_code/count_h5.py:15) with 2 output partitions (allowLocal=false)\n",
      "15/11/29 07:08:56 INFO scheduler.DAGScheduler: Final stage: Stage 0(count at /data/asoto/projectW205/test_code/count_h5.py:15)\n",
      "15/11/29 07:08:56 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "15/11/29 07:08:56 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "15/11/29 07:08:56 INFO scheduler.DAGScheduler: Submitting Stage 0 (PythonRDD[2] at count at /data/asoto/projectW205/test_code/count_h5.py:15), which has no missing parents\n",
      "15/11/29 07:08:56 INFO storage.MemoryStore: ensureFreeSpace(5968) called with curMem=298095, maxMem=278302556\n",
      "15/11/29 07:08:56 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 5.8 KB, free 265.1 MB)\n",
      "15/11/29 07:08:56 INFO storage.MemoryStore: ensureFreeSpace(3840) called with curMem=304063, maxMem=278302556\n",
      "15/11/29 07:08:56 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.8 KB, free 265.1 MB)\n",
      "15/11/29 07:08:56 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:59005 (size: 3.8 KB, free: 265.4 MB)\n",
      "15/11/29 07:08:56 INFO storage.BlockManagerMaster: Updated info of block broadcast_1_piece0\n",
      "15/11/29 07:08:56 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:839\n",
      "15/11/29 07:08:56 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 0 (PythonRDD[2] at count at /data/asoto/projectW205/test_code/count_h5.py:15)\n",
      "15/11/29 07:08:56 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 2 tasks\n",
      "15/11/29 07:08:56 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1372 bytes)\n",
      "15/11/29 07:08:56 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1372 bytes)\n",
      "15/11/29 07:08:56 INFO executor.Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
      "15/11/29 07:08:56 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "15/11/29 07:08:56 INFO executor.Executor: Fetching file:/data/asoto/projectW205/test_code/count_h5.py with timestamp 1448780934952\n",
      "15/11/29 07:08:56 INFO util.Utils: /data/asoto/projectW205/test_code/count_h5.py has been previously copied to /tmp/spark-51519a1b-a3bb-4cab-8661-87ddb6129223/userFiles-61316a33-874d-43e9-b8cc-0fd3e27f7cff/count_h5.py\n",
      "15/11/29 07:08:57 INFO rdd.HadoopRDD: Input split: file:/data/asoto/projectW205/data/list_files.txt:0+289999\n",
      "15/11/29 07:08:57 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/11/29 07:08:57 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/11/29 07:08:57 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/11/29 07:08:57 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/11/29 07:08:57 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/11/29 07:08:57 INFO rdd.HadoopRDD: Input split: file:/data/asoto/projectW205/data/list_files.txt:289999+290000\n"
     ]
    }
   ],
   "source": [
    "!spark-submit test_code/count_h5.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
