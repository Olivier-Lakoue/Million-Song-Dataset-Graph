{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Model of the Million Song Dataset\n",
    "\n",
    "*Andrea Soto*  \n",
    "*MIDS W205 Final Project*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "This notebook processess the Million Song Dataset Subset which contains a sample of 10,000 songs from the main dataset. **The goal is to get familiarized with the data, and to develop and test the code on the smaller dataset.** The code developed was then compiled in scripts that were used to process the full dataset.\n",
    "\n",
    "Two sources of data were used:\n",
    "\n",
    "1. **Million Song Dataset (MSD):** contains track, song, artist, and album metadata. It also contains artist similarity and artist tags. This was the main datasource for this project. The data is stored in HDF5 format, with one file per song.\n",
    "A detail description of the MSD project can be found [here](http://labrosa.ee.columbia.edu/millionsong/) and the filed list can be found [here](http://labrosa.ee.columbia.edu/millionsong/faq).\n",
    "2. **Last.fm Dataset:** contains information about song similarity and song tags. This information was used to complement the MSD. This data is stored in JSON format, with one file per song. A detail desctiption of the Last.fm data can be found [here](http://labrosa.ee.columbia.edu/millionsong/lastfm).\n",
    "\n",
    "For reference, a small sample of the tree structue of each dataset is shown below.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data/MillionSongSubset/data/\n",
    "|-- A\n",
    "|   |-- A\n",
    "|   |   |-- A\n",
    "|   |   |   |-- TRAAAAW128F429D538.h5\n",
    "|   |   |   |-- TRAAABD128F429CF47.h5\n",
    "|   |   |   |-- TRAAADZ128F9348C2E.h5\n",
    "|   |   |   |-- ...\n",
    "|   |   |-- B\n",
    "|   |   |   |-- TRAABCL128F4286650.h5\n",
    "|   |   |   |-- TRAABDL12903CAABBA.h5\n",
    "|   |   |   |-- TRAABJL12903CDCF1A.h5\n",
    "|   |   |   |-- ...\n",
    "|   |   |-- C\n",
    "|   |   |   |-- TRAACCG128F92E8A55.h5\n",
    "|   |   |   |-- TRAACER128F4290F96.h5\n",
    "|   |   |   |-- TRAACFV128F935E50B.h5\n",
    "|   |   |   |-- ...\n",
    "|   |   |-- D\n",
    "|   |   |-- ...\n",
    "|   |   |-- X\n",
    "|   |   |-- Y\n",
    "|   |   `-- Z\n",
    "|   |-- B\n",
    "|   |   |-- A\n",
    "|   |   |-- ...\n",
    "|   |   `-- Z\n",
    "|   |-- C\n",
    "|   |-- ...\n",
    "|   |-- Z\n",
    "`-- B\n",
    "    |-- A\n",
    "    |   |-- A\n",
    "    |   |-- B\n",
    "    |   |-- ...\n",
    "    |   |-- Z\n",
    "    |-- B\n",
    "    |-- ...\n",
    "    |-- Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data/lastfm_subset/\n",
    "|-- A\n",
    "|   |-- A\n",
    "|   |   |-- A\n",
    "|   |   |   |-- TRAAAAW128F429D538.json\n",
    "|   |   |   |-- TRAAABD128F429CF47.json\n",
    "|   |   |   |-- TRAAADZ128F9348C2E.json\n",
    "|   |   |   |-- ...\n",
    "|   |   |-- B\n",
    "|   |   |   |-- TRAABDL12903CAABBA.json\n",
    "|   |   |   |-- TRAABJL12903CDCF1A.json\n",
    "|   |   |   |-- TRAABJV128F1460C49.json\n",
    "|   |   |   |-- ...\n",
    "|   |   |-- C\n",
    "|   |   |   |-- TRAACCG128F92E8A55.json\n",
    "|   |   |   |-- TRAACER128F4290F96.json\n",
    "|   |   |   |-- TRAACFV128F935E50B.json\n",
    "|   |   |   |-- ...\n",
    "|   |   |-- D\n",
    "|   |   |-- ...\n",
    "|   |   |-- X\n",
    "|   |   |-- Y\n",
    "|   |   `-- Z\n",
    "|   |-- B\n",
    "|   |   |-- A\n",
    "|   |   |-- ...\n",
    "|   |   `-- Z\n",
    "|   |-- C\n",
    "|   |-- ...\n",
    "|   |-- Z\n",
    "`-- B\n",
    "    |-- A\n",
    "    |   |-- A\n",
    "    |   |-- B\n",
    "    |   |-- ...\n",
    "    |   |-- Z\n",
    "    |-- B\n",
    "    |-- ...\n",
    "    |-- Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Workflow\n",
    "\n",
    "The Million Song Dataset is stored in HDF5 files. The data was transformed into CSV files containing the nodes and realtionship structure for Neo4j. This CSV files where created to leaverage the `LOAD CVS` functionality of Neo4j, which makes loading large graphs into Neo4j faster and scalable. \n",
    "\n",
    "The steps followed in this notebook were:\n",
    "\n",
    "1. Create a list of the song files with the full path to each file\n",
    "2. In Spark, read each file in the list and extract the information\n",
    "3. In Spark, transform the extracted data to create CSV files for each node and relationsip type. Separate transformations were requiered for each node and relationsip type\n",
    "4. Save the transformed data using Spark's `saveAsTextFile()` operation\n",
    "5. Since the `saveAsTextFile()` operation generates several files named 'part-000xx', the data was merged into a single .csv file and headers were added to ease readability. This was done with bash commands\n",
    "\n",
    "The MSD team found some matching errors between tracks and songs in the data. They created a list of (song id, tack id) pairs that are not trusted and they suggest removing this pairs from the data. These missmatches were removed from the data as part of the transformation process.\n",
    "\n",
    "For more details see:\n",
    "- http://labrosa.ee.columbia.edu/millionsong/blog/12-1-2-matching-errors-taste-profile-and-msd\n",
    "- http://labrosa.ee.columbia.edu/millionsong/blog/12-2-12-fixing-matching-errors\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LINK TO DOWNLOAD NEO4J\n",
    "\n",
    "http://neo4j.com/artifact.php?name=neo4j-community-2.3.1-unix.tar.gz\n",
    " \n",
    "#### AWS SAMPLE CODE\n",
    "\n",
    "https://alestic.com/2013/11/aws-cli-query/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt to automate configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "# W205 Final Project: Million Song Dataset (MSD)\n",
    "\n",
    "# Requirements: W205 AMI with Hadoop and Spark\n",
    "#               aws cli installed and configured ()\n",
    "# This configurations scripts is run from within the EC2 instance.\n",
    "# It assumes that the instance DOES NOT have any volume attached and that the mount\n",
    "# point /data is available\n",
    " \n",
    "# Python Libraries: py2neo,\n",
    "#'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "# AS ROOT\n",
    "# === Installations ===\n",
    "sudo yum install jq\n",
    "pip install awscli\n",
    " \n",
    "# === Install ec2-metadata tool to get information about this instance ===\n",
    "wget http://s3.amazonaws.com/ec2metadata/ec2-metadata\n",
    "chmod a+x ec2-metadata\n",
    "mv ec2-metadata /usr/bin\n",
    " \n",
    "# ============================================================================================================================\n",
    "#'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "# AWS Setup - Attache 2 volumes to this instance\n",
    "# Main Volume:\n",
    "# MSD VolumeL  is where the Million Song Dataset (MSD)\n",
    "#'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "# === Save instance info in environment variables ===\n",
    "# Get instance id\n",
    "INSTANCE_ID=$(ec2-metadata -i | cut -d:  -f2| cut -d' ' -f2)\n",
    "# Get instance public hostname\n",
    "INSTANCE_PDNS=$(ec2-metadata -p | cut -d:  -f2| cut -d' ' -f2)\n",
    "# Get instance availability zone\n",
    "INSTANCE_ZONE=$(ec2-metadata -z | cut -d:  -f2| cut -d' ' -f2)\n",
    "\n",
    "\n",
    "export INSTANCE_ID\n",
    "export INSTANCE_PDNS\n",
    "export INSTANCE_ZONE\n",
    "\n",
    "echo 'export INSTANCE_ID='$INSTANCE_ID >> ~/.bashrc\n",
    "echo 'export INSTANCE_PDNS='$INSTANCE_PDNS >> ~/.bashrc\n",
    "echo 'export INSTANCE_ZONE='$INSTANCE_ZONE >> ~/.bashrc\n",
    "\n",
    "source ~/.bashrc\n",
    "\n",
    "# === Create and Attache Volumes ===\n",
    "mkdir aws-info\n",
    " \n",
    "### Create project main working volume\n",
    "aws ec2 create-volume --size 100 --availability-zone $INSTANCE_ZONE --volume-type gp2 > aws-info/main-volume.json\n",
    "wait\n",
    "MAIN_VOL_ID = jq '.VolumeId' aws-info/main-volume.json\n",
    " \n",
    "aws ec2 attach-volume --volume-id $MAIN_VOL_ID --instance-id $INSTANCE_ID --device /dev/xvdf\n",
    "wait\n",
    "mkdir data\n",
    "sudo mkfs -t ext4 /dev/xvdf\n",
    "sudo mount /dev/xvdf /data\n",
    " \n",
    "### Create volume from AWS snapshot of Million Song Dataset (full dataset)\n",
    "aws ec2 create-volume --availability-zone $INSTANCE_ZONE --snapshot-id snap-5178cf30 --volume-type gp2 > aws-info/msd-volume.json\n",
    "wait\n",
    "MSD_VOL_ID = jq '.VolumeId' aws-info/msd-volume.json\n",
    " \n",
    "aws ec2 attach-volume --volume-id $MSD_VOL_ID --instance-id $INSTANCE_ID --device /dev/xvdg\n",
    "wait\n",
    "mkdir msong_data\n",
    "sudo mount /dev/xvdg /msong_data\n",
    " \n",
    "# ============================================================================================================================\n",
    "# === Install Neo4j in main directory ===\n",
    " \n",
    "cd /data\n",
    "wget http://neo4j.com/artifact.php?name=neo4j-community-2.3.1-unix.tar.gz\n",
    "tar -xf neo4j-community-2.3.1-unix.tar.gz\n",
    "export NEO4J_HOME=\"/data/neo4j\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Subset Data - 10,000 songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a project directory\n",
    "!mkdir msd_project\n",
    "!cd msd_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile download_subsetdata.sh\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "#Create a directory for the data\n",
    "mkdir data\n",
    "cd data\n",
    "\n",
    "# Download data subset of 10,000 songs, ~1GB to develop and test code\n",
    "wget http://static.echonest.com/millionsongsubset_full.tar.gz data_subset\n",
    "wait\n",
    "\n",
    "tar xvzf millionsongsubset_full.tar.gz\n",
    "wait\n",
    "\n",
    "# Download list of all artist ID \n",
    "# The format is: artist id<SEP>artist mbid<SEP>track id<SEP>artist name\n",
    "wget http://labrosa.ee.columbia.edu/millionsong/sites/default/files/AdditionalFiles/unique_artists.txt\n",
    "wait\n",
    "wc -l unique_artists.txt #44745 unique_artists.txt\n",
    "\n",
    "# Download list of all unique artist terms (Echo Nest tags) \n",
    "wget http://labrosa.ee.columbia.edu/millionsong/sites/default/files/AdditionalFiles/unique_terms.txt\n",
    "wait\n",
    "wc -l unique_terms.txt #7643 unique_terms.txt\n",
    "    \n",
    "# Download list of all unique artist musicbrainz tags\n",
    "wget http://labrosa.ee.columbia.edu/millionsong/sites/default/files/AdditionalFiles/unique_mbtags.txt\n",
    "wait\n",
    "wc -l unique_mbtags.txt #2321 unique_mbtags.txt\n",
    "\n",
    "# Download last-fm data with song similarities\n",
    "wget http://labrosa.ee.columbia.edu/millionsong/sites/default/files/lastfm/lastfm_subset.zip\n",
    "unzip lastfm_subset.zip\n",
    "\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download untrusted Songs to be filtered out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2015-12-12 21:37:41--  http://labrosa.ee.columbia.edu/millionsong/sites/default/files/tasteprofile/sid_mismatches.txt\n",
      "Resolving labrosa.ee.columbia.edu... 128.59.66.11\n",
      "Connecting to labrosa.ee.columbia.edu|128.59.66.11|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2026182 (1.9M) [text/plain]\n",
      "Saving to: `sid_mismatches.txt'\n",
      "\n",
      "100%[======================================>] 2,026,182   8.42M/s   in 0.2s    \n",
      "\n",
      "2015-12-12 21:37:41 (8.42 MB/s) - `sid_mismatches.txt' saved [2026182/2026182]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wget http://labrosa.ee.columbia.edu/millionsong/sites/default/files/tasteprofile/sid_mismatches.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Spark Context in Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.5.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.10 (default, Sep 15 2015 14:50:01)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "#Escape L for line numbers\n",
    "spark_home = os.environ['SPARK_HOME'] = '/data/spark15'\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "execfile(os.path.join(spark_home,'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f0ee8089710>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check spark context exists\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import h5py\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list of HDF5 files and JSON files for the Subset Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run from 'msd_project' directory\n",
    "# List the files with their full path and store the list in a .txt file\n",
    "# This list of files will then be read in Spark to parse the actual files\n",
    "\n",
    "# '''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''' \n",
    "# Million Song Dataset\n",
    "# ''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "# Path to list of song files - Million Song Subset - \n",
    "song_paths = 'data/list_hdf5_files.txt'\n",
    "\n",
    "# If file does not exits, create it\n",
    "if not os.path.exists(song_paths):\n",
    "\n",
    "    # List all paths of songs and save them to \n",
    "    get_song_paths = glob.glob('./data/MillionSongSubset/data/*/*/*/*.h5')\n",
    "    \n",
    "    with open(song_paths,'w') as f:\n",
    "        f.writelines('\\n'.join(p for p in get_song_paths))\n",
    "        f.close()\n",
    "\n",
    "        \n",
    "# '''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''' \n",
    "# Last.fm Dataset\n",
    "# ''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "# Path to list of song files - LastFM Song Similarity and Tags -\n",
    "lastfm_paths = 'data/list_lastfm_files.txt'\n",
    "\n",
    "# If file does not exits, create it\n",
    "if not os.path.exists(lastfm_paths):\n",
    "\n",
    "    # List all paths of songs and save them to \n",
    "    get_song_paths = glob.glob('./data/lastfm_subset/*/*/*/*.json')\n",
    "    \n",
    "    with open(lastfm_paths,'w') as f:\n",
    "        f.writelines('\\n'.join(p for p in get_song_paths))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read list of files to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdir = os.getcwd()\n",
    "\n",
    "# Create RDD with the list of HDF5 song files\n",
    "path = os.path.join(cdir, song_paths)\n",
    "song_pathsRDD = sc.textFile('file://'+path)\n",
    "\n",
    "# Create RDD with the list of JSON song files\n",
    "path = os.path.join(cdir, lastfm_paths)\n",
    "lastfm_pathsRDD = sc.textFile('file://'+path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample list of MSD song files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'./data/MillionSongSubset/data/B/B/O/TRBBOPX12903D106F7.h5',\n",
       " u'./data/MillionSongSubset/data/B/B/O/TRBBOKQ128F933AE7C.h5',\n",
       " u'./data/MillionSongSubset/data/B/B/O/TRBBOPV12903CFB50F.h5']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_pathsRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample list of Last.fm song files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'./data/lastfm_subset/B/B/O/TRBBOBO128F425FDFC.json',\n",
       " u'./data/lastfm_subset/B/B/O/TRBBOPX12903D106F7.json',\n",
       " u'./data/lastfm_subset/B/B/O/TRBBOBQ12903CC5186.json']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lastfm_pathsRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the (song id, track id) pair mismatches\n",
    "\n",
    "The raw mismatched file has the following general structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ERROR: <'songID', 'trackID'> 'descrption showing mismatch'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some example lines from the file are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ERROR: <SOVWUNG12A8C137891 TRMGMLW128F426A200> Warlock  -  Copy of a Copy  !=  Sickboy  -  She's out of way  \n",
    "ERROR: <SOJTFZA12A8C13704E TRMGGOK128F426FDEB> Bike  -  Circus Kids  !=  Slut  -  Gloom  \n",
    "ERROR: <SOZZXCP12A8C13832E TRMGQMW128F9311251> Musiq  -  Solong  !=  Suthun Boy  -  Full Blown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_mismatches(line):\n",
    "    '''\n",
    "    This function extracts the songID and trackID of the mismatched records.\n",
    "    Returned value: ('songID', 'trackID')\n",
    "    '''\n",
    "    return line[8:45].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an RDD with the song-track pairs that need to be removed\n",
    "toRemoveRDD = sc.textFile('file://'+cdir+'/sid_mismatches.txt').map(parse_mismatches)\n",
    "songsToRemove = sc.broadcast(toRemoveRDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of (song,track) pairs to remove**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19094"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toRemoveRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract MSD song information\n",
    "\n",
    "The MSD song data was extracted into an RDD were each song is represendted with a python dictionary having the following structure:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{   'a_similar': array(['artistId', 'artistId', ... , 'artistId']),  \n",
    "      'a_terms': array(['term1', 'term2', ..., 'termN']),  \n",
    "       'a_tfrq': array([ ]),  \n",
    "         'a_tw': array([ ]),  \n",
    "        'album': 'album name',  \n",
    "  'artist_7did': '7digit artist id',  \n",
    "    'artist_id': 'Echo Nest artist id',  \n",
    "  'artist_mbid': 'Music Brain artist id',  \n",
    "  'artist_name': 'Artist name',  \n",
    "        'dance': 0.0,  \n",
    "          'dur': 125.7,  \n",
    "       'energy': 0.0,  \n",
    "     'loudness': -9.3,  \n",
    "      'song_id': 'Echo Nest song id',  \n",
    "        'title': 'Song title',  \n",
    "     'track_id': 'Echo Nest trach id',  \n",
    "         'year': 1990  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to read MSD HDF5 files and extract data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_h5_info(path):\n",
    "    '''\n",
    "    Takes a path to a song stored as an HDF5 file and returns a dictionary with the \n",
    "    information that will be included in the graph\n",
    "    ''' \n",
    "    d = {}\n",
    "    with h5py.File(path, 'r') as f:\n",
    "\n",
    "        # --- Artist Info -----------------------------\n",
    "        d.setdefault('artist_id', f['metadata']['songs']['artist_id'][0])\n",
    "        d.setdefault('artist_mbid', f['metadata']['songs']['artist_mbid'][0])\n",
    "        d.setdefault('artist_7did', f['metadata']['songs']['artist_7digitalid'][0])\n",
    "        d.setdefault('artist_name', f['metadata']['songs']['artist_name'][0])\n",
    "\n",
    "        # --- Song Info -----------------------------\n",
    "        d.setdefault('song_id', f['metadata']['songs']['song_id'][0])\n",
    "        d.setdefault('track_id', f['analysis']['songs']['track_id'][0])\n",
    "        d.setdefault('title', f['metadata']['songs']['title'][0])\n",
    "        d.setdefault('dance', f['analysis']['songs']['danceability'][0])\n",
    "        d.setdefault('dur', f['analysis']['songs']['duration'][0])\n",
    "        d.setdefault('energy', f['analysis']['songs']['energy'][0])\n",
    "        d.setdefault('loudness', f['analysis']['songs']['loudness'][0])\n",
    "\n",
    "        # --- Year -----------------------------\n",
    "        d.setdefault('year', f['musicbrainz']['songs']['year'][0])\n",
    "\n",
    "        # --- Album -----------------------------\n",
    "        d.setdefault('album', f['metadata']['songs']['release'][0])\n",
    "\n",
    "        # --- Similar Artist -----------------------------\n",
    "        d.setdefault('a_similar', np.array(f['metadata']['similar_artists']))\n",
    "\n",
    "        # --- Artist Terms -----------------------------\n",
    "        d.setdefault('a_terms', np.array(f['metadata']['artist_terms']))\n",
    "        d.setdefault('a_tfrq', np.array(f['metadata']['artist_terms_freq']))\n",
    "        d.setdefault('a_tw', np.array(f['metadata']['artist_terms_weight']))\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract MSD song data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract song information\n",
    "songsRDD = song_pathsRDD.map(get_h5_info).filter(\n",
    "    lambda x: [x['song_id'],x['track_id']] not in songsToRemove.value).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample song extracted**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "songsRDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Last.fm song information\n",
    "\n",
    "The Last.fm data has the following structure:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{    'artist': u'DeGarmo & Key',\n",
    "   'similars': [['song id', similarity measure], ['song id', similarity measure]],\n",
    "       'tags': [['tag one', weight],['some tag', weight]],\n",
    "  'timestamp': '2011-09-08 01:41:45.776631',\n",
    "      'title': 'Jericho  (Straight On Album Version)',\n",
    "   'track_id': 'TRBBOBO128F425FDFC'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to read Last.fm JSON files and extract data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_json_info(path):\n",
    "    with open(path) as data_file:    \n",
    "        return json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract song information\n",
    "lastfmRDD = lastfm_pathsRDD.map(get_json_info).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample song**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'artist': u'DeGarmo & Key',\n",
       "  u'similars': [],\n",
       "  u'tags': [],\n",
       "  u'timestamp': u'2011-09-08 01:41:45.776631',\n",
       "  u'title': u'Jericho  (Straight On Album Version)',\n",
       "  u'track_id': u'TRBBOBO128F425FDFC'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lastfmRDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table shows the nodes that will be created\n",
    "\n",
    "|Node Label|File Name| Format |\n",
    "|:--|:--|:--|\n",
    "|Artists|nodes_artists.csv| 'artist_id', 'artist_mbid', 'artist_7did', 'artist_name'|\n",
    "|Songs|nodes_songs.csv|'song_id', 'track_id', 'title', 'dance', 'dur', 'energy','loudness'|\n",
    "|Albums|nodes_albums.csv| 'album_name'|\n",
    "|Year|nodes_years.csv| 'year'|\n",
    "|Tags|nodes_tags.csv| 'tag'|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artist Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create artist nodes\n",
    "fields = ['artist_id', 'artist_mbid', 'artist_7did', 'artist_name']\n",
    "fieldsBrC = sc.broadcast(fields)\n",
    "\n",
    "outputfile = os.path.join(cdir,'data/MillionSongSubset/tmp/nodes_artists')\n",
    "# If directory already exists, delete it\n",
    "if os.path.exists(outputfile):\n",
    "    shutil.rmtree(outputfile)\n",
    "\n",
    "songsRDD.map(makeCSVline).distinct().saveAsTextFile('file://'+outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Song Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create song nodes\n",
    "fields = ['song_id', 'track_id', 'title', 'dance', 'dur', 'energy','loudness']\n",
    "fieldsBrC = sc.broadcast(fields)\n",
    "\n",
    "outputfile = os.path.join(cdir,'data/MillionSongSubset/tmp/nodes_songs')\n",
    "if os.path.exists(outputfile):\n",
    "    shutil.rmtree(outputfile)\n",
    "\n",
    "songsRDD.map(makeCSVline).distinct().saveAsTextFile('file://'+outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Album Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create album nodes\n",
    "outputfile = os.path.join(cdir,'data/MillionSongSubset/tmp/nodes_albums')\n",
    "if os.path.exists(outputfile):\n",
    "    shutil.rmtree(outputfile)\n",
    "\n",
    "songsRDD.map(lambda x: x['album']).distinct().saveAsTextFile('file://'+outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Year Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create year nodes\n",
    "outputfile = os.path.join(cdir,'data/MillionSongSubset/tmp/nodes_years')\n",
    "if os.path.exists(outputfile):\n",
    "    shutil.rmtree(outputfile)\n",
    "    \n",
    "songsRDD.map(lambda x: x['year']).filter(lambda x: int(x) > 0).distinct().saveAsTextFile('file://'+outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag Nodes\n",
    "\n",
    "The MSD data containes artist tags and  the Last.fm data containes song tags.\n",
    "\n",
    "The tags in the dataset have a large overlap. For example, the tags 'pop' and 'rock' are used to describe both artists and songs. Since the tags can be the same and convey the same information, I decided to model tags as one type of node. \n",
    "\n",
    "The tags were merged together and then the list of tags was created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'songsRDD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-aec24d6fa15d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#songsRDD.flatMap(lambda x: x['a_terms']).distinct().saveAsTextFile('file://'+outputfile)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0martistTags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msongsRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'a_terms'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'songsRDD' is not defined"
     ]
    }
   ],
   "source": [
    "# Tag nodes\n",
    "outputfile = os.path.join(cdir,'data/MillionSongSubset/tmp/nodes_tags')\n",
    "if os.path.exists(outputfile):\n",
    "    shutil.rmtree(outputfile)\n",
    "    \n",
    "#songsRDD.flatMap(lambda x: x['a_terms']).distinct().saveAsTextFile('file://'+outputfile)\n",
    "\n",
    "artistTags = songsRDD.flatMap(lambda x: x['a_terms']).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "songTags = lastfmRDD.flatMap(lambda x: x['tags']).map(lambda x: x[0]).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'indie', u'100'],\n",
       " [u'chinese female vocal', u'25'],\n",
       " [u'just4lov', u'25'],\n",
       " [u'of christmas past', u'25']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lastfmRDD.filter(lambda x: x['tags']<>[]).flatMap(lambda x: x['tags']).take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'indie',\n",
       " u'chinese female vocal',\n",
       " u'just4lov',\n",
       " u'of christmas past',\n",
       " u'sweetodd',\n",
       " u'in china gibt es doch musik',\n",
       " u'suicide on your stereo set',\n",
       " u'Volltonfarbes Lieblingslieder',\n",
       " u'popLove',\n",
       " u'dinlemeye kiyamiyorum']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lastfmRDD.flatMap(lambda x: x['tags']).map(lambda x: x[0]).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table shows the relationships that will be created\n",
    "\n",
    "|Relationship Structure|File Name| Format |\n",
    "|:--|:--|:--|\n",
    "|(ARTIST) - [SIMILAR_TO] -> (ARTIST)|rel_similar_artists.csv|'from_artist_id', 'to_artist_id'|\n",
    "|(ARTIST) - [PERFORMS] -> (SONG)|rel_performs.csv|'artist_id', 'song_id'|\n",
    "|(ARTIST) - [HAS_ALBUM] -> (ALBUM)|rel_artist_has_album.csv|'artist_id', 'album_name'|\n",
    "|(SONG) - [IN_ALBUM] -> (ALBUM)|rel_song_in_album.csv|'song_id', 'album_name'|\n",
    "|(ARTIST) - [HAS_TAG] -> (TAG)|rel_artist_tag.csv|'artist_id', 'tag_name', 'normalized_frq', 'normalized_weight'|\n",
    "|(SONG) - [SIMILAR_TO] -> (SONG)| rel_similar_songs.csv|'from_song_id', 'to_song_id', 'similarity_weight'|\n",
    "\n",
    "# Missing Relationships\n",
    "\n",
    "|Relationship Structure|File Name| Format |\n",
    "|:--|:--|:--|\n",
    "|(SONG) - [HAS_TAG] -> (TAG)| rel_song_tag.csv|'from_song_id', 'to_song_id', 'normalized_weight'|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HAS_TAG relationship between artist and tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def artistToTags(record):\n",
    "    '''\n",
    "    Concatenate artist with each tag\n",
    "    Normalize tag frequency and weight\n",
    "    '''\n",
    "    normalize_frq = record['a_tfrq'] / sum(record['a_tfrq'])\n",
    "    normalize_w = record['a_tw'] / sum(record['a_tw'])\n",
    "    terms = record['a_terms']\n",
    "    artist = record['artist_id']\n",
    "    \n",
    "    result = []\n",
    "    for i in range(len(terms)):\n",
    "        result.append( artist +\",\"+ terms[i] +\",\"+ str(normalize_frq[i]) +\",\"+ str(normalize_w[i]))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Artist Has Tags (edge has properties)\n",
    "outputfile = os.path.join(cdir,'data/MillionSongSubset/tmp/rel_artist_tag')\n",
    "if os.path.exists(outputfile):\n",
    "    shutil.rmtree(outputfile)\n",
    "\n",
    "songsRDD.flatMap(artistToTags).distinct().saveAsTextFile('file://'+outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMILAR_TO relationship between artist and artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Similar Artist to Artist (directional, no properties)\n",
    "outputfile = os.path.join(cdir,'data/MillionSongSubset/tmp/rel_similar_artists')\n",
    "if os.path.exists(outputfile):\n",
    "    shutil.rmtree(outputfile)\n",
    "    \n",
    "similarArtistsRDD = songsRDD.map(lambda x: (x['artist_id'],x['a_similar'])).flatMapValues(lambda x: x)\n",
    "similarArtistsRDD.distinct().map(lambda x: x[0]+\",\"+x[1]).saveAsTextFile('file://'+outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PERFORMS relationship between artist and song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Artist Performs Song\n",
    "outputfile = os.path.join(cdir,'data/MillionSongSubset/tmp/rel_performs')\n",
    "if os.path.exists(outputfile):\n",
    "    shutil.rmtree(outputfile)\n",
    "    \n",
    "songsRDD.map(lambda x: x['artist_id']+\",\"+x['song_id']).distinct().saveAsTextFile('file://'+outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HAS_ALBUM relationship between artist and album"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Artist Has Album\n",
    "outputfile = os.path.join(cdir,'data/MillionSongSubset/tmp/rel_artist_has_album')\n",
    "if os.path.exists(outputfile):\n",
    "    shutil.rmtree(outputfile)\n",
    "    \n",
    "songsRDD.map(lambda x: x['artist_id']+\",\"+x['album']).distinct().saveAsTextFile('file://'+outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IN_ALBUM relationship between song and album"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Song In Album\n",
    "outputfile = os.path.join(cdir,'data/MillionSongSubset/tmp/rel_song_in_album')\n",
    "if os.path.exists(outputfile):\n",
    "    shutil.rmtree(outputfile)\n",
    "    \n",
    "songsRDD.map(lambda x: x['song_id']+\",\"+x['album']).distinct().saveAsTextFile('file://'+outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMILAR_TO relationship between song and song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Similar Song to Song\n",
    "# .csv format: from_track_id, to_track_id, similarity_measure\n",
    "outputfile = os.path.join(cdir,'data/MillionSongSubset/tmp/rel_similar_songs')\n",
    "if os.path.exists(outputfile):\n",
    "    shutil.rmtree(outputfile)\n",
    "    \n",
    "similarSongsRDD = lastfmRDD.filter(\n",
    "    lambda x: x['similars']<>[]).map(\n",
    "    lambda x: (x['track_id'],x['similars'])).flatMapValues(lambda x: x)\n",
    "similarSongsRDD.map(lambda x: x[0]+\",\"+x[1][0]+\",\"+str(x[1][1])).saveAsTextFile('file://'+outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Spark output \n",
    "\n",
    "Spark outputs several files named 'part-000xx' which cannot be read into Neo4j. To load the data to Neo4j, I combined the Spark output into a .csv file which can be imported to Neo4j. Additionally, a header line was added for better readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('cat data/MillionSongSubset/tmp/nodes_artists/part-* > data/MillionSongSubset/graph/nodes_artists.csv')\n",
    "os.system('cat data/MillionSongSubset/tmp/nodes_songs/part-* > data/MillionSongSubset/graph/nodes_songs.csv')\n",
    "os.system('cat data/MillionSongSubset/tmp/nodes_albums/part-* > data/MillionSongSubset/graph/nodes_albums.csv')\n",
    "os.system('cat data/MillionSongSubset/tmp/nodes_years/part-* > data/MillionSongSubset/graph/nodes_years.csv')\n",
    "os.system('cat data/MillionSongSubset/tmp/nodes_tags/part-* > data/MillionSongSubset/graph/nodes_tags.csv')\n",
    "\n",
    "sed -i '1iname' nodes_albums.csv\n",
    "sed -i '1iid,mbid,7did,name' nodes_artists.csv\n",
    "sed -i '1iid,trackid,name,dance,dur,energy,loudness' nodes_songs.csv\n",
    "sed -i '1itag' nodes_tags.csv\n",
    "sed -i '1iyear' nodes_years.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('cat data/MillionSongSubset/tmp/rel_artist_tag/part-* > data/MillionSongSubset/graph/rel_artist_tag.csv')\n",
    "os.system('cat data/MillionSongSubset/tmp/rel_similar_artists/part-* > data/MillionSongSubset/graph/rel_similar_artists.csv')\n",
    "os.system('cat data/MillionSongSubset/tmp/rel_performs/part-* > data/MillionSongSubset/graph/rel_performs.csv')\n",
    "os.system('cat data/MillionSongSubset/tmp/rel_artist_has_album/part-* > data/MillionSongSubset/graph/rel_artist_has_album.csv')\n",
    "os.system('cat data/MillionSongSubset/tmp/rel_song_in_album/part-* > data/MillionSongSubset/graph/rel_song_in_album.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECK THAT HEADERS ARE THE SAME AS USED TO IMPORT DATA TO NEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sed -i '1iname' nodes_albums.csv\n",
    "sed -i '1iid,mbid,7did,name' nodes_artists.csv\n",
    "sed -i '1iid,trackid,name,dance,dur,energy,loudness' nodes_songs.csv\n",
    "sed -i '1itag' nodes_tags.csv\n",
    "sed -i '1iyear' nodes_years.csv\n",
    "\n",
    "sed -i '1iartistId,album' rel_artist_has_album.csv  \n",
    "sed -i '1iartist_id,tag_name,frq,weight' rel_artist_tag.csv  \n",
    "sed -i '1iartist,song' rel_performs.csv  \n",
    "sed -i '1ifrom,to' rel_similar_artists.csv  \n",
    "sed -i '1isongID,album' rel_song_in_album.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Structure for general script where sparkcontext has to be created and run with `spark-sbmit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile test_code/count_h5.py\n",
    "#!/usr/bin/env python\n",
    "from pyspark import SparkContext\n",
    "import time\n",
    "import h5py\n",
    "\n",
    "def read_h5_file(path):\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        return f['metadata']['songs']['title'][0]\n",
    "#Start Time\n",
    "t1 = time.time()\n",
    "\n",
    "# --- Process files ----\n",
    "sc = SparkContext(appName=\"SparkHDF5\")\n",
    "file_paths = sc.textFile('file:///data/asoto/projectW205/data/list_files.txt')\n",
    "\n",
    "songs = file_paths.map(read_h5_file)\n",
    "songs.count()\n",
    "# ----------------------\n",
    "\n",
    "#End Time\n",
    "t2 = time.time()\n",
    "sec = t2-t1\n",
    "\n",
    "print \"Run Time: %0.2f sec = %.2f min = %.2f h\"%(sec,sec/60.0,sec/1440.0)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!spark-submit test_code/count_h5.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
