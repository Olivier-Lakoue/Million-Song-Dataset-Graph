{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create general scripts to process entire dataset\n",
    "\n",
    "*Andrea Soto*  \n",
    "*MIDS W205 Final Project*  \n",
    "*Project Name: Graph Model of the Million Song Dataset*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(glob.glob('MillionSongSubset/*/*/*/*.h5') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists('MillionSongSubset/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Create a list of HDF5 and JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/list_MDS_files.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/list_MDS_files.py\n",
    "#!/usr/bin/env python\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "def main(inFile, outFile, overwrite = False):\n",
    "    \n",
    "    try:\n",
    "        os.path.exists(inFile)\n",
    "    except:\n",
    "        print \"Input file: '%s' does not exist\"%(inFile)\n",
    "    else:\n",
    "        outFile = outFile + '/list_hdf5_files.txt'\n",
    "        if not os.path.exists(outFile) or overwrite:\n",
    "            # List all paths of songs\n",
    "            get_song_paths = glob.glob(inFile+'/*/*/*/*.h5')\n",
    "            \n",
    "            if not get_song_paths:\n",
    "                print \"No HDF5 (.h5) files foung in '%s'\"%(inFile)\n",
    "                print \"Check that the file structure under '%s' is /*/*/*/song_files.h5\"%(inFile)\n",
    "            else:\n",
    "                with open(outFile,'w') as f:\n",
    "                    f.writelines('\\n'.join(p for p in get_song_paths))\n",
    "                    f.close()\n",
    "                print \"File '%s' successfully created\"%(outFile)\n",
    "        else:\n",
    "            print \"File '%s' already exists\"%(outFile)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    Creates the file 'list_hdf5_files.txt' with the list of HDF5 files\n",
    "    \n",
    "    USE:\n",
    "    python list_MDS_files.py <path to songs> <save list path> <OPTIONAL overwrite>\n",
    "    \n",
    "    Paths should NOT include '/' at the end\n",
    "    If the file already exists, it will not be overwritten. Send 'True' to overwrite\n",
    "    '''\n",
    "    \n",
    "    input_path = sys.argv[1]  \n",
    "    output_path = sys.argv[2]\n",
    "    \n",
    "    # Option to overwrite current file\n",
    "    overwrite = False\n",
    "    if len(sys.argv) > 3:\n",
    "        overwrite  = sys.argv[3]\n",
    "    \n",
    "    main(input_path, output_path, overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing scripts/list_LastFM_files.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/list_LastFM_files.py\n",
    "#!/usr/bin/env python\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "def main(inFile, outFile, overwrite = False):\n",
    "    \n",
    "    try:\n",
    "        os.path.exists(inFile)\n",
    "    except:\n",
    "        print \"Input file: '%s' does not exist\"%(inFile)\n",
    "    else:\n",
    "        outFile = outFile + '/list_lastfm_files.txt'\n",
    "        if not os.path.exists(outFile) or overwrite:\n",
    "            # List all paths of songs\n",
    "            get_song_paths = glob.glob(inFile+'/*/*/*/*.json')\n",
    "            \n",
    "            if not get_song_paths:\n",
    "                print \"No JSON files foung in '%s'\"%(inFile)\n",
    "                print \"Check that the file structure under '%s' is /*/*/*/song_files.json\"%(inFile)\n",
    "            else:\n",
    "                with open(outFile,'w') as f:\n",
    "                    f.writelines('\\n'.join(p for p in get_song_paths))\n",
    "                    f.close()\n",
    "                print  \"File '%s' successfully created\"%(outFile)\n",
    "        else:\n",
    "            print \"File '%s' already exists\"%(outFile)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    Creates the file 'list_lastfm_files.txt' with the list of HDF5 files\n",
    "    \n",
    "    USE:\n",
    "    python list_MDS_files.py <path to songs> <save list path> <OPTIONAL overwrite>\n",
    "    \n",
    "    Paths should NOT include '/' at the end\n",
    "    If the file already exists, it will not be overwritten. Send 'True' to overwrite\n",
    "    '''\n",
    "    input_path = sys.argv[1]  \n",
    "    output_path = sys.argv[2]\n",
    "    \n",
    "    # Option to overwrite current file\n",
    "    overwrite = False\n",
    "    if len(sys.argv) > 3:\n",
    "        overwrite  = sys.argv[3]\n",
    "    \n",
    "    main(input_path, output_path, overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'test/list_hdf5_files.txt' successfully created\r\n"
     ]
    }
   ],
   "source": [
    "!python scripts/list_MDS_files.py MillionSongSubset/data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'test/list_lastfm_files.txt' successfully created\r\n"
     ]
    }
   ],
   "source": [
    "!python scripts/list_LastFM_files.py MillionSongSubset/lastfm_subset test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1068\r\n",
      "-rw-rw-r-- 1 asoto asoto 509999 Dec 14 07:01 list_hdf5_files.txt\r\n",
      "-rw-rw-r-- 1 asoto asoto 578459 Dec 14 07:13 list_lastfm_files.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Load and Transform Data in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile scripts/list_LastFM_files.py\n",
    "#!/usr/bin/env python\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import shutil\n",
    "from pyspark import SparkContext\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "def main(inFile, outFile, mismatchFile):\n",
    "    \n",
    "    # === Start Spark Context ===\n",
    "    sc = SparkContext(appName=\"SparkProcessing\")\n",
    "    \n",
    "    # === Load mismatches ===\n",
    "    toRemoveRDD = sc.textFile('file://'+mismatchFile+'/sid_mismatches.txt').map(parse_mismatches)\n",
    "    songsToRemove = sc.broadcast(toRemoveRDD.collect())\n",
    "    \n",
    "    # === Load list of files ===\n",
    "    song_pathsRDD   = sc.textFile('file://' + inFile + '/list_hdf5_files.txt')\n",
    "    lastfm_pathsRDD = sc.textFile('file://' + inFile + '/list_lastfm_files.txt')\n",
    "    \n",
    "    # === Extract Song Data ===\n",
    "    songsRDD = song_pathsRDD.map(get_h5_info).cache()\n",
    "    lastfmRDD = lastfm_pathsRDD.map(get_json_info).cache()\n",
    "    \n",
    "    # === Create Nodes ===\n",
    "    \n",
    "    # === ARTISTS ===\n",
    "    # CSV Format: artist_id, artist_mb_id, artist_7d_id, artist_name\n",
    "    fields = ['artist_id', 'artist_mbid', 'artist_7did', 'artist_name']\n",
    "    fieldsBrC = sc.broadcast(fields)\n",
    "    # If directory already exists, delete it\n",
    "    if os.path.exists(outFile+'/nodes_artists'):\n",
    "        shutil.rmtree(outFile+'/nodes_artists')\n",
    "    # Process and save\n",
    "    songsRDD.map(makeCSVline).distinct().saveAsTextFile('file://'+outFile+'/nodes_artists')\n",
    "    \n",
    "    # ----------------------\n",
    "\n",
    "    #End Time\n",
    "    t2 = time.time()\n",
    "    sec = t2-t1\n",
    "\n",
    "    # === Stop Spark Context ===\n",
    "    sc.stop()\n",
    "\n",
    "\n",
    "def parse_mismatches(line):\n",
    "    '''\n",
    "    This function extracts the songID and trackID of the mismatched records.\n",
    "    Returned value: ('songID', 'trackID')\n",
    "    '''\n",
    "    return line[8:45].split()\n",
    "\n",
    "\n",
    "def get_h5_info(path):\n",
    "    '''\n",
    "    Takes a path to a song stored as an HDF5 file and returns a dictionary with the \n",
    "    information that will be included in the graph\n",
    "    ''' \n",
    "    d = {}\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        song_id = f['metadata']['songs']['song_id'][0]\n",
    "        track_id = f['analysis']['songs']['track_id'][0]\n",
    "        \n",
    "        if (song_id, track_id) not in songsToRemove.value:\n",
    "\n",
    "            # --- Artist Info -----------------------------\n",
    "            d.setdefault('artist_id', f['metadata']['songs']['artist_id'][0])\n",
    "            d.setdefault('artist_mbid', f['metadata']['songs']['artist_mbid'][0])\n",
    "            d.setdefault('artist_7did', f['metadata']['songs']['artist_7digitalid'][0])\n",
    "            d.setdefault('artist_name', f['metadata']['songs']['artist_name'][0])\n",
    "\n",
    "            # --- Song Info -----------------------------\n",
    "            d.setdefault('song_id', song_id)\n",
    "            d.setdefault('track_id', track_id)\n",
    "            d.setdefault('title', f['metadata']['songs']['title'][0])\n",
    "            d.setdefault('dance', f['analysis']['songs']['danceability'][0])\n",
    "            d.setdefault('dur', f['analysis']['songs']['duration'][0])\n",
    "            d.setdefault('energy', f['analysis']['songs']['energy'][0])\n",
    "            d.setdefault('loudness', f['analysis']['songs']['loudness'][0])\n",
    "\n",
    "            # --- Year -----------------------------\n",
    "            d.setdefault('year', f['musicbrainz']['songs']['year'][0])\n",
    "\n",
    "            # --- Album -----------------------------\n",
    "            d.setdefault('album', f['metadata']['songs']['release'][0])\n",
    "\n",
    "            # --- Similar Artist -----------------------------\n",
    "            d.setdefault('a_similar', np.array(f['metadata']['similar_artists']))\n",
    "\n",
    "            # --- Artist Terms -----------------------------\n",
    "            d.setdefault('a_terms', np.array(f['metadata']['artist_terms']))\n",
    "            d.setdefault('a_tfrq', np.array(f['metadata']['artist_terms_freq']))\n",
    "            d.setdefault('a_tw', np.array(f['metadata']['artist_terms_weight']))\n",
    "\n",
    "            return d\n",
    "        else: \n",
    "            pass\n",
    "\n",
    "def get_json_info(path):\n",
    "    with open(path) as data_file:    \n",
    "        return json.load(data_file)\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    input_path: path to where the list of hdf5 and json files was created\n",
    "    output_path: a temporary directory where the Spark CSV files separated as part-000xx files will be stored\n",
    "    mismatch_path: path to where the mismatches file is located\n",
    "    \n",
    "    DO NOT INCLUDE '/' AT THE END OF PATH\n",
    "    Cannot change file names\n",
    "    '''\n",
    "    input_path = sys.argv[1]  \n",
    "    output_path = sys.argv[2]\n",
    "    mismatch_path = sys.argv[2]\n",
    "    \n",
    "    \n",
    "    main(input_path, output_path, mismatch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "import h5py\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "# General script structure with spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile test_code/count_h5.py\n",
    "#!/usr/bin/env python\n",
    "from pyspark import SparkContext\n",
    "import time\n",
    "import h5py\n",
    "\n",
    "def read_h5_file(path):\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        return f['metadata']['songs']['title'][0]\n",
    "#Start Time\n",
    "t1 = time.time()\n",
    "\n",
    "# --- Process files ----\n",
    "sc = SparkContext(appName=\"SparkHDF5\")\n",
    "file_paths = sc.textFile('file:///data/asoto/projectW205/data/list_files.txt')\n",
    "\n",
    "songs = file_paths.map(read_h5_file)\n",
    "songs.count()\n",
    "# ----------------------\n",
    "\n",
    "#End Time\n",
    "t2 = time.time()\n",
    "sec = t2-t1\n",
    "\n",
    "print \"Run Time: %0.2f sec = %.2f min = %.2f h\"%(sec,sec/60.0,sec/1440.0)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!spark-submit test_code/count_h5.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
