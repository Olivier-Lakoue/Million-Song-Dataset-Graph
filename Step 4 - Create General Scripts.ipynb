{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create general scripts to process entire dataset\n",
    "\n",
    "*Andrea Soto*  \n",
    "*MIDS W205 Final Project*  \n",
    "*Project Name: Graph Model of the Million Song Dataset*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Create a list of HDF5 and JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/list_MDS_files.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/list_MDS_files.py\n",
    "#!/usr/bin/env python\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "def main(inDir, outDir, overwrite = False):\n",
    "    \n",
    "    try:\n",
    "        os.path.exists(inDir)\n",
    "    except:\n",
    "        print \"Input file: '%s' does not exist\"%(inDir)\n",
    "    else:\n",
    "        outFile = outDir + '/list_hdf5_files.txt'\n",
    "        if not os.path.exists(outFile) or overwrite:\n",
    "            # List all paths of songs\n",
    "            get_song_paths = glob.glob(inDir+'/*/*/*/*.h5')\n",
    "            \n",
    "            if not get_song_paths:\n",
    "                print \"No HDF5 (.h5) files foung in '%s'\"%(inDir)\n",
    "                print \"Check that the file structure under '%s' is /*/*/*/song_files.h5\"%(inDir)\n",
    "            else:\n",
    "                with open(outFile,'w') as f:\n",
    "                    f.writelines('\\n'.join(p for p in get_song_paths))\n",
    "                    f.close()\n",
    "                print \"File '%s' successfully created\"%(outFile)\n",
    "        else:\n",
    "            print \"File '%s' already exists\"%(outFile)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    Creates the file 'list_hdf5_files.txt' with the list of HDF5 files\n",
    "    \n",
    "    USE:\n",
    "    python list_MDS_files.py <path to songs> <save list path> <OPTIONAL overwrite>\n",
    "    \n",
    "    Paths should NOT include '/' at the end\n",
    "    If the file already exists, it will not be overwritten. Send 'True' to overwrite\n",
    "    '''\n",
    "    \n",
    "    input_path = sys.argv[1]  \n",
    "    output_path = sys.argv[2]\n",
    "    \n",
    "    # Option to overwrite current file\n",
    "    overwrite = False\n",
    "    if len(sys.argv) > 3:\n",
    "        overwrite  = sys.argv[3]\n",
    "    \n",
    "    main(input_path, output_path, overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/list_LastFM_files.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/list_LastFM_files.py\n",
    "#!/usr/bin/env python\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "def main(inDir, outDir, overwrite = False):\n",
    "    \n",
    "    try:\n",
    "        os.path.exists(inDir)\n",
    "    except:\n",
    "        print \"Input file: '%s' does not exist\"%(inDir)\n",
    "    else:\n",
    "        outFile = outDir + '/list_lastfm_files.txt'\n",
    "        if not os.path.exists(outFile) or overwrite:\n",
    "            # List all paths of songs\n",
    "            get_song_paths = glob.glob(inDir+'/*/*/*/*.json')\n",
    "            \n",
    "            if not get_song_paths:\n",
    "                print \"No JSON files foung in '%s'\"%(inDir)\n",
    "                print \"Check that the file structure under '%s' is /*/*/*/song_files.json\"%(inDir)\n",
    "            else:\n",
    "                with open(outFile,'w') as f:\n",
    "                    f.writelines('\\n'.join(p for p in get_song_paths))\n",
    "                    f.close()\n",
    "                print  \"File '%s' successfully created\"%(outFile)\n",
    "        else:\n",
    "            print \"File '%s' already exists\"%(outFile)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    Creates the file 'list_lastfm_files.txt' with the list of HDF5 files\n",
    "    \n",
    "    USE:\n",
    "    python list_MDS_files.py <path to songs> <save list path> <OPTIONAL overwrite>\n",
    "    \n",
    "    Paths should NOT include '/' at the end\n",
    "    If the file already exists, it will not be overwritten. Send 'True' to overwrite\n",
    "    '''\n",
    "    input_path = sys.argv[1]  \n",
    "    output_path = sys.argv[2]\n",
    "    \n",
    "    # Option to overwrite current file\n",
    "    overwrite = False\n",
    "    if len(sys.argv) > 3:\n",
    "        overwrite  = sys.argv[3]\n",
    "    \n",
    "    main(input_path, output_path, overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'test/list_hdf5_files.txt' successfully created\r\n"
     ]
    }
   ],
   "source": [
    "!python scripts/list_MDS_files.py MillionSongSubset/data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'test/list_lastfm_files.txt' successfully created\r\n"
     ]
    }
   ],
   "source": [
    "!python scripts/list_LastFM_files.py MillionSongSubset/lastfm_subset test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1068\r\n",
      "-rw-rw-r-- 1 asoto asoto 509999 Dec 14 07:01 list_hdf5_files.txt\r\n",
      "-rw-rw-r-- 1 asoto asoto 578459 Dec 14 07:13 list_lastfm_files.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Load and Transform Data in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/extractData.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/extractData.py\n",
    "#!/usr/bin/env python\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import shutil\n",
    "from pyspark import SparkContext\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "def main(inDir, outDir, mismatchFile):\n",
    "    \n",
    "    # === Start Spark Context ===\n",
    "    sc = SparkContext(appName=\"SparkProcessing\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # === Load mismatches ===\n",
    "    toRemoveRDD = sc.textFile('file://'+mismatchFile+'/sid_mismatches.txt').map(parse_mismatches)\n",
    "    songsToRemove = sc.broadcast(toRemoveRDD.collect())\n",
    "    \n",
    "    # === Load list of files ===\n",
    "    song_pathsRDD   = sc.textFile('file://' + inDir + '/list_hdf5_files.txt')\n",
    "    lastfm_pathsRDD = sc.textFile('file://' + inDir + '/list_lastfm_files.txt')\n",
    "    \n",
    "    # === Extract Song Data ===\n",
    "    songsRDD = song_pathsRDD.map(get_h5_info).cache()\n",
    "    lastfmRDD = lastfm_pathsRDD.map(get_json_info).cache()\n",
    "    \n",
    "    # =====================================================================\n",
    "    # == Delete Sub-Folders ===\n",
    "    folders = ['/nodes_artists','/nodes_songs','/nodes_albums','/nodes_years','/nodes_tags',\n",
    "              '/rel_similar_artists', '/rel_performs','/rel_artist_has_album','/rel_artist_has_tag',\n",
    "               '/rel_song_in_album','/rel_similar_songs', '/rel_song_has_tag', '/rel_song_year']\n",
    "    for p in folders:\n",
    "        if os.path.exists(outDir+p):\n",
    "            shutil.rmtree(outDir+p)\n",
    "    \n",
    "    # =====================================================================\n",
    "    # === Create Nodes ===\n",
    "    \n",
    "    # === ARTISTS ===\n",
    "    # CSV Format: artist_id, artist_mb_id, artist_7d_id, artist_name\n",
    "    fields = ['artist_id', 'artist_mbid', 'artist_7did', 'artist_name']\n",
    "    fieldsBrC = sc.broadcast(fields)\n",
    "    songsRDD.map(makeCSVline).distinct().saveAsTextFile('file://'+outDir+'/nodes_artists')\n",
    "    \n",
    "    # === SONGS ===\n",
    "    # CSV Format: song_id, track_id, song_title, danceability, duration, energy, loudness\n",
    "    fields = ['song_id', 'track_id', 'title', 'dance', 'dur', 'energy','loudness']\n",
    "    fieldsBrC = sc.broadcast(fields)\n",
    "    songsRDD.map(makeCSVline).distinct().saveAsTextFile('file://'+outDir+'/nodes_songs')\n",
    "    \n",
    "    # === ALBUMS ===\n",
    "    # CSV Format: album_name\n",
    "    songsRDD.map(lambda x: x['album']).distinct().saveAsTextFile('file://'+outDir+'/nodes_albums')\n",
    "    \n",
    "    # === YEAR ===\n",
    "    # CSV Format: year\n",
    "    songsRDD.map(lambda x: x['year']).filter(\n",
    "        lambda x: int(x) > 0).distinct().saveAsTextFile('file://'+outDir+'/nodes_years')\n",
    "    \n",
    "    # === TAGS ===\n",
    "    # CSV Format: tag_name\n",
    "    artistTags = songsRDD.flatMap(lambda x: x['a_terms']).distinct()\n",
    "    songTags = lastfmRDD.flatMap(lambda x: x['tags']).map(lambda x: x[0]).distinct()\n",
    "    allTags = songTags.union(artistTags).distinct()\n",
    "    allTags.saveAsTextFile('file://'+outDir+'/nodes_tags')\n",
    "    \n",
    "    # =====================================================================\n",
    "    # === Create Relationships ===\n",
    "    \n",
    "    # === SIMILAR_TO relationship between artist and artist ===\n",
    "    # CSV Format: from_artist_id, to_artist_id\n",
    "    # Similar Artist to Artist (directional, no properties)\n",
    "    similarArtistsRDD = songsRDD.map(lambda x: (x['artist_id'],x['a_similar'])).flatMapValues(lambda x: x)\n",
    "    similarArtistsRDD.distinct().map(lambda x: x[0]+\",\"+x[1]).saveAsTextFile('file://'+outDir+'/rel_similar_artists')\n",
    "    \n",
    "    # === PERFORMS relationship between artist and song ===\n",
    "    # CSV Format: artist_id, song_id\n",
    "    # Artist Performs Song (directional, no properties)\n",
    "    songsRDD.map(lambda x: x['artist_id']+\",\"+x['song_id']).distinct().saveAsTextFile('file://'+outDir+'/rel_performs')\n",
    "    \n",
    "    # === HAS_ALBUM relationship between artist and album  ===\n",
    "    # CSV Format: artist_id, album_name\n",
    "    # Artist Has Album (directional, no properties)\n",
    "    songsRDD.map(lambda x: x['artist_id']+\",\"+x['album']).distinct().saveAsTextFile('file://'+outDir+'/rel_artist_has_album')\n",
    "\n",
    "    # === HAS_TAG relationship between artist and tags ===\n",
    "    # CSV Format: artist_id, tag_name, tag_frequency, tag_weight\n",
    "    # Artist Has Tags (directional, has properties frequency and weight)\n",
    "    songsRDD.flatMap(artistToTags).distinct().saveAsTextFile('file://'+outDir+'/rel_artist_has_tag')\n",
    "    \n",
    "    # === IN_ALBUM relationship between song and album ===\n",
    "    # CSV Format: song_id, album_name\n",
    "    # Song In Album (direction, no properties)   \n",
    "    songsRDD.map(lambda x: x['song_id']+\",\"+x['album']).distinct().saveAsTextFile('file://'+outDir+'/rel_song_in_album')\n",
    "    \n",
    "    # === SIMILAR_TO relationship between song and song ===\n",
    "    #CSV Format: from_track_id, to_track_id, similarity_measure\n",
    "    # Similar Song to Song (directional, with property similarity measure)   \n",
    "    similarSongsRDD = lastfmRDD.filter(\n",
    "        lambda x: x['similars']<>[]).map(\n",
    "        lambda x: (x['track_id'],x['similars'])).flatMapValues(lambda x: x)\n",
    "    similarSongsRDD.map(\n",
    "        lambda x: x[0]+\",\"+x[1][0]+\",\"+str(x[1][1])).saveAsTextFile('file://'+outDir+'/rel_similar_songs')\n",
    "    \n",
    "    # === HAS_TAG relationship between song and tags\n",
    "    # CSV Format: track_id, tag_name, tag_weight\n",
    "    lastfmRDD.flatMap(songToTags).saveAsTextFile('file://'+outDir+'/rel_song_has_tag')\n",
    "    \n",
    "    # === RELEASED_ON relationship between song and year\n",
    "    # CSV Format: song_id, year\n",
    "    # Song Released in Year (directional, no properties)\n",
    "    songsRDD.filter(lambda x: int(x['year'])<>0).map(\n",
    "        lambda x: x['song_id']+\",\"+str(x['year'])).saveAsTextFile('file://'+outDir+'/rel_song_year')\n",
    "\n",
    "\n",
    "    # =====================================================================\n",
    "    # === Stop Spark Context ===\n",
    "    sc.stop()\n",
    "\n",
    "\n",
    "def parse_mismatches(line):\n",
    "    '''\n",
    "    This function extracts the songID and trackID of the mismatched records.\n",
    "    Returned value: ('songID', 'trackID')\n",
    "    '''\n",
    "    return line[8:45].split()\n",
    "\n",
    "\n",
    "def get_h5_info(path):\n",
    "    '''\n",
    "    Takes a path to a song stored as an HDF5 file and returns a dictionary with the \n",
    "    information that will be included in the graph\n",
    "    ''' \n",
    "    d = {}\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        song_id = f['metadata']['songs']['song_id'][0]\n",
    "        track_id = f['analysis']['songs']['track_id'][0]\n",
    "        \n",
    "        if (song_id, track_id) not in songsToRemove.value:\n",
    "\n",
    "            # --- Artist Info -----------------------------\n",
    "            d.setdefault('artist_id', f['metadata']['songs']['artist_id'][0])\n",
    "            d.setdefault('artist_mbid', f['metadata']['songs']['artist_mbid'][0])\n",
    "            d.setdefault('artist_7did', f['metadata']['songs']['artist_7digitalid'][0])\n",
    "            d.setdefault('artist_name', f['metadata']['songs']['artist_name'][0])\n",
    "\n",
    "            # --- Song Info -----------------------------\n",
    "            d.setdefault('song_id', song_id)\n",
    "            d.setdefault('track_id', track_id)\n",
    "            d.setdefault('title', f['metadata']['songs']['title'][0])\n",
    "            d.setdefault('dance', f['analysis']['songs']['danceability'][0])\n",
    "            d.setdefault('dur', f['analysis']['songs']['duration'][0])\n",
    "            d.setdefault('energy', f['analysis']['songs']['energy'][0])\n",
    "            d.setdefault('loudness', f['analysis']['songs']['loudness'][0])\n",
    "\n",
    "            # --- Year -----------------------------\n",
    "            d.setdefault('year', f['musicbrainz']['songs']['year'][0])\n",
    "\n",
    "            # --- Album -----------------------------\n",
    "            d.setdefault('album', f['metadata']['songs']['release'][0])\n",
    "\n",
    "            # --- Similar Artist -----------------------------\n",
    "            d.setdefault('a_similar', np.array(f['metadata']['similar_artists']))\n",
    "\n",
    "            # --- Artist Terms -----------------------------\n",
    "            d.setdefault('a_terms', np.array(f['metadata']['artist_terms']))\n",
    "            d.setdefault('a_tfrq', np.array(f['metadata']['artist_terms_freq']))\n",
    "            d.setdefault('a_tw', np.array(f['metadata']['artist_terms_weight']))\n",
    "\n",
    "            return d\n",
    "        else: \n",
    "            pass\n",
    "\n",
    "def get_json_info(path):\n",
    "    with open(path) as data_file:    \n",
    "        return json.load(data_file)\n",
    "\n",
    "def makeCSVline(line):\n",
    "    return ','.join(str(line[field]) for field in fieldsBrC.value)\n",
    "    \n",
    "def artistToTags(record):\n",
    "    '''\n",
    "    Concatenate artist with each tag\n",
    "    Normalize tag frequency and weight\n",
    "    '''\n",
    "    normalize_frq = record['a_tfrq'] / sum(record['a_tfrq'])\n",
    "    normalize_w = record['a_tw'] / sum(record['a_tw'])\n",
    "    terms = record['a_terms']\n",
    "    artist = record['artist_id']\n",
    "    \n",
    "    result = []\n",
    "    for i in range(len(terms)):\n",
    "        result.append( artist +\",\"+ terms[i] +\",\"+ str(normalize_frq[i]) +\",\"+ str(normalize_w[i]))\n",
    "    \n",
    "    return result\n",
    "\n",
    "def songToTags(record):\n",
    "    '''\n",
    "    Concatenate song with each tag\n",
    "    '''\n",
    "    tags = record['tags']\n",
    "    total_weight = sum(float(w[1]) for w in tags)\n",
    "    track_id = record['track_id']\n",
    "    \n",
    "    result = []\n",
    "    for i in range(len(tags)):\n",
    "        result.append( track_id +\",\"+ tags[i][0] +\",\"+ str(float(tags[i][1])/total_weight))\n",
    "    \n",
    "    return result\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    input_path: path to where the list of hdf5 and json files was created\n",
    "    output_path: a temporary directory where the Spark CSV files separated as part-000xx files will be stored\n",
    "    mismatch_path: path to where the mismatches file is located\n",
    "    \n",
    "    DO NOT INCLUDE '/' AT THE END OF PATH\n",
    "    Cannot change file names\n",
    "    '''\n",
    "    input_path = sys.argv[1]  \n",
    "    output_path = sys.argv[2]\n",
    "    mismatch_path = sys.argv[2]\n",
    "    \n",
    "    main(input_path, output_path, mismatch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "# General script structure with spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile test_code/count_h5.py\n",
    "#!/usr/bin/env python\n",
    "from pyspark import SparkContext\n",
    "import time\n",
    "import h5py\n",
    "\n",
    "def read_h5_file(path):\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        return f['metadata']['songs']['title'][0]\n",
    "#Start Time\n",
    "t1 = time.time()\n",
    "\n",
    "# --- Process files ----\n",
    "sc = SparkContext(appName=\"SparkHDF5\")\n",
    "file_paths = sc.textFile('file:///data/asoto/projectW205/data/list_files.txt')\n",
    "\n",
    "songs = file_paths.map(read_h5_file)\n",
    "songs.count()\n",
    "# ----------------------\n",
    "\n",
    "#End Time\n",
    "t2 = time.time()\n",
    "sec = t2-t1\n",
    "\n",
    "print \"Run Time: %0.2f sec = %.2f min = %.2f h\"%(sec,sec/60.0,sec/1440.0)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!spark-submit test_code/count_h5.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
