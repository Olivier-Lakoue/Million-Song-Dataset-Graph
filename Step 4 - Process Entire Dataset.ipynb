{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing entire 1,000,000 song dataset\n",
    "\n",
    "*Andrea Soto*  \n",
    "*MIDS W205 Final Project*  \n",
    "*Project Name: Graph Model of the Million Song Dataset*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "This notebook processes the entire Million Song Dataset by running the following 4 scripts sequentially:\n",
    "\n",
    "1. Create a list of .h5 files of the Million Song Dataset\n",
    "2. Create a list of .json files of the Last.fm dataset\n",
    "3. Read files, extract information, transform, and save as CSV\n",
    "\n",
    "A short exmplanation on how to use each script is provided above the code that creates the scritps.\n",
    "\n",
    "The output from this notebook will be used to upload the data into Neo4j in the notebook [Step 5 - Import Entire Dataset.ipynb](./Step 5 - Import Entire Dataset.ipynb)\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "The volume with the Million Song Dataset should be mounted on /msong_dataset, the Last.fm dataset should have been downloaded, and the file 'sid_mismatches.tx' should already exist in the 'graph/import' folder.\n",
    "\n",
    "**Updates**\n",
    "\n",
    "The scripts were created based on the work perform on previous notebooks. However, the unique id for songs was changed from 'song-id' to 'track-id' because Neo4j's import can can only create relationships based on node's unique id, and not on node properties. Since the Last.fm dataset cross-reference songs by 'track-id', this was used as the unique id of the Song Nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Create Lists of Million Song Files (.h5) and Last.fm Files (.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '/graph/import/list_hdf5_files.txt' successfully created\n",
      "File '/graph/import/list_lastfm_files.txt' successfully created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "real\t31m49.765s\n",
      "user\t0m4.842s\n",
      "sys\t0m1.536s\n",
      "\n",
      "real\t0m4.906s\n",
      "user\t0m3.938s\n",
      "sys\t0m0.960s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# python scripts/<name>.py <input_path> <output_path>\n",
    "time python scripts/list_MDS_files.py    /msong_dataset/data /graph/import\n",
    "time python scripts/list_LastFM_files.py /graph/lastfm/data /graph/import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 92020\r\n",
      "-rw-rw-r-- 1 asoto asoto 47999999 Dec 14 10:43 list_hdf5_files.txt\r\n",
      "-rw-rw-r-- 1 asoto asoto 46223365 Dec 14 10:43 list_lastfm_files.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l /graph/import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data and save in CSV format\n",
    "\n",
    "Command was run from terminal and the run time was copied below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time /usr/bin/spark-submit --master local[$(nproc)] \\\n",
    "scripts/extractData.py /graph/import /graph/import/tmp /graph/import $(nproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Time\n",
      "real\t774m15.972s\n",
      "user\t 15m30.488s\n",
      "sys \t  8m22.516s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print 'Run Time'\n",
    "print 'real\\t774m15.972s\\nuser\\t 15m30.488s\\nsys \\t  8m22.516s\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List generated directories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 52\r\n",
      "drwxrwxr-x 2 asoto asoto 4096 Dec 20 05:31 nodes_albums\r\n",
      "drwxrwxr-x 2 asoto asoto 4096 Dec 20 03:19 nodes_artists\r\n",
      "drwxrwxr-x 2 asoto asoto 4096 Dec 20 04:14 nodes_songs\r\n",
      "drwxrwxr-x 2 asoto asoto 4096 Dec 20 15:48 nodes_tags\r\n",
      "drwxrwxr-x 2 asoto asoto 4096 Dec 20 06:48 nodes_years\r\n",
      "drwxrwxr-x 2 asoto asoto 4096 Dec 20 10:39 rel_artist_has_album\r\n",
      "drwxrwxr-x 2 asoto asoto 4096 Dec 20 11:57 rel_artist_has_tag\r\n",
      "drwxrwxr-x 2 asoto asoto 4096 Dec 20 09:22 rel_performs\r\n",
      "drwxrwxr-x 2 asoto asoto 4096 Dec 20 08:07 rel_similar_artists\r\n",
      "drwxrwxr-x 2 asoto asoto 4096 Dec 20 15:54 rel_similar_songs\r\n",
      "drwxrwxr-x 2 asoto asoto 4096 Dec 20 15:54 rel_song_has_tag\r\n",
      "drwxrwxr-x 2 asoto asoto 4096 Dec 20 14:31 rel_song_in_album\r\n",
      "drwxrwxr-x 2 asoto asoto 4096 Dec 20 13:14 rel_song_year\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l /graph/import/tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Size of each directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371M\t/graph/import/tmp/rel_song_has_tag\r\n",
      "162M\t/graph/import/tmp/rel_similar_artists\r\n",
      "79M\t/graph/import/tmp/nodes_songs\r\n",
      "36M\t/graph/import/tmp/rel_performs\r\n",
      "2.6G\t/graph/import/tmp/rel_similar_songs\r\n",
      "264K\t/graph/import/tmp/nodes_years\r\n",
      "3.4M\t/graph/import/tmp/nodes_albums\r\n",
      "64M\t/graph/import/tmp/rel_artist_has_tag\r\n",
      "5.8M\t/graph/import/tmp/nodes_artists\r\n",
      "12M\t/graph/import/tmp/rel_song_year\r\n",
      "9.1M\t/graph/import/tmp/nodes_tags\r\n",
      "8.9M\t/graph/import/tmp/rel_artist_has_album\r\n",
      "38M\t/graph/import/tmp/rel_song_in_album\r\n",
      "3.4G\t/graph/import/tmp\r\n"
     ]
    }
   ],
   "source": [
    "!du -h /graph/import/tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Load CSV files to Neo4j\n",
    "\n",
    "Loading the data into Neo4j was done in the notebook [Step 5 - Import Entire Dataset.ipynb](./Step 5 - Import Entire Dataset.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Script Creation\n",
    "\n",
    "# scripts/list_MDS_files.py\n",
    "\n",
    "**Script to create a list of HDF5 files**\n",
    "\n",
    "**USAGE:**\n",
    "> `python scripts/list_MDS_files.py <path_to_msdataset> <output_path> <true/false to overwrite (default:false)>`\n",
    "\n",
    ">`<path_to_msdataset>` : path to the Million Song Dataset folder called 'data' where the directory structure /\\*/\\*/\\*/trackID.h5 begins\n",
    "\n",
    ">`<output_path>` : where the file **'list_hdf5_files.txt'** with the list of the files will be saved\n",
    "\n",
    "> `<true/false to overwrite (default:false)>` : optional parameter to overwrite **'list_hdf5_files.txt'** if it exitst. Default is set to false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/list_MDS_files.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/list_MDS_files.py\n",
    "#!/usr/bin/env python\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "def main(inDir, outDir, overwrite = False):\n",
    "    \n",
    "    try:\n",
    "        os.path.exists(inDir)\n",
    "    except:\n",
    "        print \"Input file: '%s' does not exist\"%(inDir)\n",
    "    else:\n",
    "        outFile = outDir + '/list_hdf5_files.txt'\n",
    "        if not os.path.exists(outFile) or overwrite:\n",
    "            # List all paths of songs\n",
    "            get_song_paths = glob.glob(inDir+'/*/*/*/*.h5')\n",
    "            \n",
    "            if not get_song_paths:\n",
    "                print \"No HDF5 (.h5) files foung in '%s'\"%(inDir)\n",
    "                print \"Check that the file structure under '%s' is /*/*/*/song_files.h5\"%(inDir)\n",
    "            else:\n",
    "                with open(outFile,'w') as f:\n",
    "                    f.writelines('\\n'.join(p for p in get_song_paths))\n",
    "                    f.close()\n",
    "                print \"File '%s' successfully created\"%(outFile)\n",
    "        else:\n",
    "            print \"File '%s' already exists\"%(outFile)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    Creates the file 'list_hdf5_files.txt' with the list of HDF5 files\n",
    "    \n",
    "    USE:\n",
    "    python list_MDS_files.py <path to songs> <save list path> <OPTIONAL overwrite>\n",
    "    \n",
    "    Paths should NOT include '/' at the end\n",
    "    If the file already exists, it will not be overwritten. Send 'True' to overwrite\n",
    "    '''\n",
    "    \n",
    "    input_path = sys.argv[1]  \n",
    "    output_path = sys.argv[2]\n",
    "    \n",
    "    # Option to overwrite current file\n",
    "    overwrite = False\n",
    "    if len(sys.argv) > 3:\n",
    "        overwrite  = sys.argv[3]\n",
    "    \n",
    "    main(input_path, output_path, overwrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scripts/list_LastFM_files.py\n",
    "\n",
    "**Script to create a list of JSON files**\n",
    "\n",
    "**USAGE:**\n",
    "> `python scripts/list_MDS_files.py <path_to_msdataset> <output_path> <true/false to overwrite (default:false)>`\n",
    "\n",
    ">`<path_to_msdataset>` : path to the Last.fm Dataset folder called where the directory structure /\\*/\\*/\\*/trackID.json begins\n",
    "\n",
    ">`<output_path>` : where the file **'list_lastfm_files.txt'** with the list of the files will be saved\n",
    "\n",
    "> `<true/false to overwrite (default:false)>` : optional parameter to overwrite **'list_lastfm_files.txt'** if it exitst. Default is set to false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/list_LastFM_files.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/list_LastFM_files.py\n",
    "#!/usr/bin/env python\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "def main(inDir, outDir, overwrite = False):\n",
    "    \n",
    "    try:\n",
    "        os.path.exists(inDir)\n",
    "    except:\n",
    "        print \"Input file: '%s' does not exist\"%(inDir)\n",
    "    else:\n",
    "        outFile = outDir + '/list_lastfm_files.txt'\n",
    "        if not os.path.exists(outFile) or overwrite:\n",
    "            # List all paths of songs\n",
    "            get_song_paths = glob.glob(inDir+'/*/*/*/*.json')\n",
    "            \n",
    "            if not get_song_paths:\n",
    "                print \"No JSON files foung in '%s'\"%(inDir)\n",
    "                print \"Check that the file structure under '%s' is /*/*/*/song_files.json\"%(inDir)\n",
    "            else:\n",
    "                with open(outFile,'w') as f:\n",
    "                    f.writelines('\\n'.join(p for p in get_song_paths))\n",
    "                    f.close()\n",
    "                print  \"File '%s' successfully created\"%(outFile)\n",
    "        else:\n",
    "            print \"File '%s' already exists\"%(outFile)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    Creates the file 'list_lastfm_files.txt' with the list of HDF5 files\n",
    "    \n",
    "    USE:\n",
    "    python list_MDS_files.py <path to songs> <save list path> <OPTIONAL overwrite>\n",
    "    \n",
    "    Paths should NOT include '/' at the end\n",
    "    If the file already exists, it will not be overwritten. Send 'True' to overwrite\n",
    "    '''\n",
    "    input_path = sys.argv[1]  \n",
    "    output_path = sys.argv[2]\n",
    "    \n",
    "    # Option to overwrite current file\n",
    "    overwrite = False\n",
    "    if len(sys.argv) > 3:\n",
    "        overwrite  = sys.argv[3]\n",
    "    \n",
    "    main(input_path, output_path, overwrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test listing scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'test/list_hdf5_files.txt' successfully created\r\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python scripts/list_MDS_files.py MillionSongSubset/data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'test/list_lastfm_files.txt' successfully created\r\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python scripts/list_LastFM_files.py MillionSongSubset/lastfm_subset test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1068\r\n",
      "-rw-rw-r-- 1 asoto asoto 509999 Dec 14 07:01 list_hdf5_files.txt\r\n",
      "-rw-rw-r-- 1 asoto asoto 578459 Dec 14 07:13 list_lastfm_files.txt\r\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -l test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scripts/extractData.py \n",
    "\n",
    "**Script to process data and create part-00x files that have CSV format for all nodes and relations**\n",
    "\n",
    "**USAGE:**\n",
    "\n",
    "> Used with `spark-submit`  \n",
    "\n",
    "> `/usr/bin/spark-submit --master local[<CPUs>] \\\n",
    "scripts/extractData.py <path_to_lists> <output_path> <path_to_mismatches> <CPUs>`\n",
    "\n",
    "\n",
    ">`<path_to_lists>` : path to **'list_hdf5_files.txt'** and **'list_lastfm_files.txt'** files. Both files must be placed in the same directory.\n",
    "\n",
    ">`<output_path>` : where all the CSV files for the nodes and relationships will be saved. Sub-folders will be created for the following outputs:\n",
    "\n",
    "> 1. nodes_artists  \n",
    "> 1. nodes_tags   \n",
    "> 1. nodes_albums   \n",
    "> 1. nodes_songs    \n",
    "> 1. nodes_years  \n",
    "> 1. rel_performs         \n",
    "> 1. rel_artist_has_album  \n",
    "> 1. rel_song_in_album\n",
    "> 1. rel_similar_artists  \n",
    "> 1. rel_similar_songs  \n",
    "> 1. rel_artist_has_tag   \n",
    "> 1. rel_song_has_tag   \n",
    "> 1. rel_song_year\n",
    "\n",
    "> `<path_to_mismatches>` : optional parameter to overwrite **'list_lastfm_files.txt'** if it exitst. Default is set to false\n",
    "\n",
    "> `<CPUs>` : Number of CPUs available for parallel processing. This will also be used to partition files to ensure all the CPUs are used by Spark.\n",
    "\n",
    "**Sample Usage**\n",
    "\n",
    ">`time /usr/bin/spark-submit --master local[$(nproc)] \\\n",
    "scripts/extractData.py /graph/import /graph/import/tmp /graph/import $(nproc)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\r\n"
     ]
    }
   ],
   "source": [
    "!echo $(nproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/extractData.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/extractData.py\n",
    "#!/usr/bin/env python\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import shutil\n",
    "from pyspark import SparkContext\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "def parse_mismatches(line):\n",
    "    '''\n",
    "    This function extracts the songID and trackID of the mismatched records.\n",
    "    Returned value: ('songID', 'trackID')\n",
    "    '''\n",
    "    return line[8:45].split()\n",
    "\n",
    "\n",
    "def get_h5_info(path):\n",
    "    '''\n",
    "    Takes a path to a song stored as an HDF5 file and returns a dictionary with the \n",
    "    information that will be included in the graph\n",
    "    ''' \n",
    "    d = {}\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        song_id = f['metadata']['songs']['song_id'][0]\n",
    "        track_id = f['analysis']['songs']['track_id'][0]\n",
    "        \n",
    "        if [song_id, track_id] not in songsToRemove.value:\n",
    "\n",
    "            # --- Artist Info -----------------------------\n",
    "            d.setdefault('artist_id', f['metadata']['songs']['artist_id'][0])\n",
    "            d.setdefault('artist_mbid', f['metadata']['songs']['artist_mbid'][0])\n",
    "            d.setdefault('artist_7did', f['metadata']['songs']['artist_7digitalid'][0])\n",
    "            d.setdefault('artist_name', f['metadata']['songs']['artist_name'][0])\n",
    "\n",
    "            # --- Song Info -----------------------------\n",
    "            d.setdefault('song_id', song_id)\n",
    "            d.setdefault('track_id', track_id)\n",
    "            d.setdefault('title', f['metadata']['songs']['title'][0])\n",
    "            d.setdefault('dance', f['analysis']['songs']['danceability'][0])\n",
    "            d.setdefault('dur', f['analysis']['songs']['duration'][0])\n",
    "            d.setdefault('energy', f['analysis']['songs']['energy'][0])\n",
    "            d.setdefault('loudness', f['analysis']['songs']['loudness'][0])\n",
    "\n",
    "            # --- Year -----------------------------\n",
    "            d.setdefault('year', f['musicbrainz']['songs']['year'][0])\n",
    "\n",
    "            # --- Album -----------------------------\n",
    "            d.setdefault('album', f['metadata']['songs']['release'][0])\n",
    "\n",
    "            # --- Similar Artist -----------------------------\n",
    "            d.setdefault('a_similar', np.array(f['metadata']['similar_artists']))\n",
    "\n",
    "            # --- Artist Terms -----------------------------\n",
    "            d.setdefault('a_terms', np.array(f['metadata']['artist_terms']))\n",
    "            d.setdefault('a_tfrq', np.array(f['metadata']['artist_terms_freq']))\n",
    "            d.setdefault('a_tw', np.array(f['metadata']['artist_terms_weight']))\n",
    "\n",
    "            return d\n",
    "        else: \n",
    "            pass\n",
    "\n",
    "def get_json_info(path):\n",
    "    with open(path) as data_file:    \n",
    "        return json.load(data_file)\n",
    "\n",
    "def makeCSVline(line):\n",
    "    return ','.join(str(line[f]) for f in fieldsBrC.value)\n",
    "    \n",
    "def artistToTags(record):\n",
    "    '''\n",
    "    Concatenate artist with each tag\n",
    "    Normalize tag frequency and weight\n",
    "    '''\n",
    "    normalize_frq = record['a_tfrq'] / sum(record['a_tfrq'])\n",
    "    normalize_w = record['a_tw'] / sum(record['a_tw'])\n",
    "    terms = record['a_terms']\n",
    "    artist = record['artist_id']\n",
    "    \n",
    "    result = []\n",
    "    for i in range(len(terms)):\n",
    "        result.append( artist +\",\"+ terms[i] +\",\"+ str(normalize_frq[i]) +\",\"+ str(normalize_w[i]))\n",
    "    \n",
    "    return result\n",
    "\n",
    "def songToTags(record):\n",
    "    '''\n",
    "    Concatenate song with each tag\n",
    "    '''\n",
    "    tags = record['tags']\n",
    "    total_weight = sum(float(w[1]) for w in tags)\n",
    "    track_id = record['track_id']\n",
    "    \n",
    "    result = []\n",
    "    for i in range(len(tags)):\n",
    "        if (tags[i][0] <> None) and (tags[i][0] <> ''):\n",
    "            result.append( track_id +\",\"+ tags[i][0] +\",\"+ str(float(tags[i][1])/total_weight))\n",
    "    \n",
    "    return result\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    input_path: path to where the list of hdf5 and json files was created\n",
    "    output_path: a temporary directory where the Spark CSV files separated as part-000xx files will be stored\n",
    "    mismatch_path: path to where the mismatches file is located\n",
    "    \n",
    "    DO NOT INCLUDE '/' AT THE END OF PATH\n",
    "    Cannot change file names\n",
    "    '''\n",
    "    \n",
    "    inDir = sys.argv[1]  \n",
    "    outDir = sys.argv[2]\n",
    "    mismatch_path = sys.argv[3]\n",
    "    cpus = int(sys.argv[4])\n",
    "    \n",
    "    #main(input_path, output_path)\n",
    "    # === Start Spark Context ===\n",
    "    sc = SparkContext(appName=\"SparkProcessing\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # === Load mismatches ===\n",
    "    toRemoveRDD = sc.textFile('file://'+mismatch_path+'/sid_mismatches.txt',cpus).map(parse_mismatches)\n",
    "    #songsToRemove = toRemoveRDD.collect()\n",
    "    songsToRemove = sc.broadcast(toRemoveRDD.collect())\n",
    "    \n",
    "    \n",
    "    # =====================================================================\n",
    "    # === Load list of files ====== Extract Song Data ===\n",
    "    song_pathsRDD   = sc.textFile('file://' + inDir + '/list_hdf5_files.txt',cpus)\n",
    "    lastfm_pathsRDD = sc.textFile('file://' + inDir + '/list_lastfm_files.txt',cpus)\n",
    "    \n",
    "    # === Extract Song Data ===\n",
    "    songsRDD = song_pathsRDD.map(get_h5_info).filter(lambda x: x<>None).cache()\n",
    "    \n",
    "    \n",
    "    # =====================================================================\n",
    "    # == Delete Sub-Folders ===\n",
    "    folders = ['/nodes_artists','/nodes_songs','/nodes_albums','/nodes_years','/nodes_tags',\n",
    "              '/rel_similar_artists', '/rel_performs','/rel_artist_has_album','/rel_artist_has_tag',\n",
    "               '/rel_song_in_album','/rel_similar_songs', '/rel_song_has_tag', '/rel_song_year']\n",
    "    for p in folders:\n",
    "        if os.path.exists(outDir+p):\n",
    "            shutil.rmtree(outDir+p)\n",
    "    \n",
    "    \n",
    "    # === ARTISTS ===\n",
    "    # CSV Format: artist_id, artist_mb_id, artist_7d_id, artist_name\n",
    "    fields = ['artist_id', 'artist_mbid', 'artist_7did', 'artist_name']\n",
    "    fieldsBrC = sc.broadcast(fields)\n",
    "    songsRDD.map(makeCSVline).distinct().saveAsTextFile('file://'+outDir+'/nodes_artists')\n",
    "    \n",
    "    # === SONGS ===\n",
    "    # CSV Format: song_id, track_id, song_title, danceability, duration, energy, loudness\n",
    "    fields = ['song_id', 'track_id', 'title', 'dance', 'dur', 'energy','loudness']\n",
    "    fieldsBrC = sc.broadcast(fields)\n",
    "    songsRDD.map(makeCSVline).distinct().saveAsTextFile('file://'+outDir+'/nodes_songs')\n",
    "    \n",
    "    # === ALBUMS ===\n",
    "    # CSV Format: album_name\n",
    "    songsRDD.map(lambda x: x['album']).distinct().saveAsTextFile('file://'+outDir+'/nodes_albums')\n",
    "    \n",
    "    # === YEAR ===\n",
    "    # CSV Format: year\n",
    "    songsRDD.map(lambda x: x['year']).filter(\n",
    "        lambda x: int(x) > 0).distinct().saveAsTextFile('file://'+outDir+'/nodes_years')\n",
    "\n",
    "    \n",
    "    # === SIMILAR_TO relationship between artist and artist ===\n",
    "    # CSV Format: from_artist_id, to_artist_id\n",
    "    # Similar Artist to Artist (directional, no properties)\n",
    "    similarArtistsRDD = songsRDD.map(lambda x: (x['artist_id'],x['a_similar'])).flatMapValues(lambda x: x)\n",
    "    similarArtistsRDD.distinct().map(lambda x: x[0]+\",\"+x[1]).saveAsTextFile('file://'+outDir+'/rel_similar_artists')\n",
    "    \n",
    "    # === PERFORMS relationship between artist and song ===\n",
    "    # CSV Format: artist_id, song_id\n",
    "    # Artist Performs Song (directional, no properties)\n",
    "    songsRDD.map(lambda x: x['artist_id']+\",\"+x['track_id']).distinct().saveAsTextFile('file://'+outDir+'/rel_performs')\n",
    "    \n",
    "    # === HAS_ALBUM relationship between artist and album  ===\n",
    "    # CSV Format: artist_id, album_name\n",
    "    # Artist Has Album (directional, no properties)\n",
    "    songsRDD.map(lambda x: x['artist_id']+\",\"+x['album']).distinct().saveAsTextFile('file://'+outDir+'/rel_artist_has_album')\n",
    "\n",
    "    # === HAS_TAG relationship between artist and tags ===\n",
    "    # CSV Format: artist_id, tag_name, tag_frequency, tag_weight\n",
    "    # Artist Has Tags (directional, has properties frequency and weight)\n",
    "    songsRDD.flatMap(artistToTags).distinct().saveAsTextFile('file://'+outDir+'/rel_artist_has_tag')\n",
    "    \n",
    "    # === RELEASED_ON relationship between song and year\n",
    "    # CSV Format: song_id, year\n",
    "    # Song Released in Year (directional, no properties)\n",
    "    songsRDD.filter(lambda x: int(x['year'])<>0).map(\n",
    "        lambda x: x['track_id']+\",\"+str(x['year'])).saveAsTextFile('file://'+outDir+'/rel_song_year')\n",
    "    \n",
    "    # === IN_ALBUM relationship between song and album ===\n",
    "    # CSV Format: song_id, album_name\n",
    "    # Song In Album (direction, no properties)   \n",
    "    songsRDD.map(lambda x: x['track_id']+\",\"+x['album']).distinct().saveAsTextFile('file://'+outDir+'/rel_song_in_album')\n",
    "    \n",
    "    # === Extract Lastfm Song Data ===\n",
    "    lastfmRDD = lastfm_pathsRDD.map(get_json_info).cache()\n",
    "    \n",
    "    # === TAGS ===\n",
    "    # CSV Format: tag_name\n",
    "    artistTags = songsRDD.flatMap(lambda x: x['a_terms']).distinct()\n",
    "    songTags = lastfmRDD.flatMap(lambda x: x['tags']).map(lambda x: x[0]).distinct()\n",
    "    allTags = songTags.union(artistTags).distinct()\n",
    "    allTags.saveAsTextFile('file://'+outDir+'/nodes_tags')\n",
    "    \n",
    "    # === SIMILAR_TO relationship between song and song ===\n",
    "    #CSV Format: from_track_id, to_track_id, similarity_measure\n",
    "    # Similar Song to Song (directional, with property similarity measure)   \n",
    "    similarSongsRDD = lastfmRDD.filter(\n",
    "        lambda x: x['similars']<>[]).map(\n",
    "        lambda x: (x['track_id'],x['similars'])).flatMapValues(lambda x: x)\n",
    "    similarSongsRDD.map(\n",
    "        lambda x: x[0]+\",\"+x[1][0]+\",\"+str(x[1][1])).saveAsTextFile('file://'+outDir+'/rel_similar_songs')\n",
    "    \n",
    "    # === HAS_TAG relationship between song and tags\n",
    "    # CSV Format: track_id, tag_name, tag_weight\n",
    "    lastfmRDD.flatMap(songToTags).saveAsTextFile('file://'+outDir+'/rel_song_has_tag')\n",
    "    \n",
    "    # === Stop Spark Context ===\n",
    "    sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
