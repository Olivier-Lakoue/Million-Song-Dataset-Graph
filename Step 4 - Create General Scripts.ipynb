{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create general scripts to process entire dataset\n",
    "\n",
    "*Andrea Soto*  \n",
    "*MIDS W205 Final Project*  \n",
    "*Project Name: Graph Model of the Million Song Dataset*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where I got how to instal h5py in EMR cluster\n",
    "http://myjourneyasadatascientist.com/tag/ec2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '/graph/import/list_hdf5_files.txt' successfully created\n",
      "File '/graph/import/list_lastfm_files.txt' successfully created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "real\t31m49.765s\n",
      "user\t0m4.842s\n",
      "sys\t0m1.536s\n",
      "\n",
      "real\t0m4.906s\n",
      "user\t0m3.938s\n",
      "sys\t0m0.960s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "time python scripts/list_MDS_files.py    /msong_dataset/data /graph/import\n",
    "time python scripts/list_LastFM_files.py /graph/lastfm/data /graph/import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                                                                                            (0 + 2) / 2]\r",
      "[Stage 0:==============================================================>                                                              (1 + 1) / 2]\r",
      "                                                                                                                                                  \r",
      "\r",
      "[Stage 1:>                                                                                                                            (0 + 2) / 2]\r",
      "[Stage 1:>                                                                                                                            (0 + 2) / 2]\r",
      "[Stage 2:>                                                                                                                            (0 + 0) / 2]\r",
      "                                                                                                                                                  \r",
      "\r",
      "[Stage 4:>                                                                                                                            (0 + 0) / 2]\r",
      "                                                                                                                                                  \r",
      "\r",
      "[Stage 8:>                                                                                                                            (0 + 0) / 2]\r",
      "                                                                                                                                                  \r",
      "\r",
      "[Stage 9:>                                                   (0 + 2) / 2][Stage 10:>                                                  (0 + 2) / 2]\r",
      "[Stage 9:>                                                                                                                            (0 + 2) / 2]\r",
      "[Stage 12:>                                                                                                                           (0 + 0) / 4]\r",
      "                                                                                                                                                  \r",
      "\r",
      "[Stage 13:>                                                                                                                           (0 + 2) / 2]\r",
      "[Stage 13:==============================================================>                                                             (1 + 1) / 2]\r",
      "[Stage 14:>                                                                                                                           (0 + 2) / 2]\r",
      "                                                                                                                                                  \r",
      "\r",
      "[Stage 16:>                                                                                                                           (0 + 0) / 2]\r",
      "                                                                                                                                                  \r",
      "\r",
      "[Stage 18:>                                                                                                                           (0 + 0) / 2]\r",
      "                                                                                                                                                  \r",
      "\r",
      "[Stage 19:>                                                                                                                           (0 + 2) / 2]\r",
      "                                                                                                                                                  \r",
      "\r",
      "[Stage 22:>                                                                                                                           (0 + 0) / 2]\r",
      "                                                                                                                                                  \r",
      "\r",
      "[Stage 23:>                                                                                                                           (0 + 2) / 2]\r",
      "                                                                                                                                                  \r",
      "\r",
      "[Stage 24:>                                                                                                                           (0 + 2) / 2]\r",
      "                                                                                                                                                  \r\n",
      "real\t1m59.394s\n",
      "user\t0m24.290s\n",
      "sys\t0m7.831s\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "# scripts/extractData.py <input_path> <output_path> <mismatch_path>\n",
    "time /usr/bin/spark-submit --master local[8] \\\n",
    "scripts/extractData.py /data/asoto/projectW205/test /data/asoto/projectW205/test/tmp /data/asoto/projectW205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 92020\r\n",
      "-rw-rw-r-- 1 asoto asoto 47999999 Dec 14 10:43 list_hdf5_files.txt\r\n",
      "-rw-rw-r-- 1 asoto asoto 46223365 Dec 14 10:43 list_lastfm_files.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l /graph/import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is terminated.\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "# scripts/extractData.py <input_path> <output_path> <mismatch_path>\n",
    "time /usr/bin/spark-submit --master local[8] \\\n",
    "scripts/extractData.py /graph/import /graph/import/tmp /data/asoto/projectW205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "time /usr/bin/spark-submit --master local[32] \\\n",
    "/graph/W205_FinalProject/scripts/simpleExtract.py /graph/import /graph/import/tmp /graph/import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real 774m15.972s\n",
    "user  15m30.488s\n",
    "sys    8m22.516s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMR Cluster\n",
    "\n",
    "**Bootstrap script to install h5py in EMR cluster (s3://asoto/bootstrap_msd_setup.sh)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env bash\n",
    "\n",
    "sudo pip install pyspark\n",
    "sudo pip install cython\n",
    "sudo pip install numpy\n",
    "sudo pip install numexpr\n",
    "\n",
    "wget http://www.hdfgroup.org/ftp/HDF5/current/src/hdf5-1.8.16.tar\n",
    "tar -xvf hdf5-1.8.16.tar \n",
    "cd hdf5-1.8.16\n",
    "./configure -prefix=/usr/local\n",
    "make\n",
    "sudo make install\n",
    "\n",
    "cd ..\n",
    "sudo pip install h5py\n",
    "sudo pip install unittest2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# scripts/list_MDS_files.py\n",
    "\n",
    "**Script to create a list of HDF5 files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/list_MDS_files.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/list_MDS_files.py\n",
    "#!/usr/bin/env python\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "def main(inDir, outDir, overwrite = False):\n",
    "    \n",
    "    try:\n",
    "        os.path.exists(inDir)\n",
    "    except:\n",
    "        print \"Input file: '%s' does not exist\"%(inDir)\n",
    "    else:\n",
    "        outFile = outDir + '/list_hdf5_files.txt'\n",
    "        if not os.path.exists(outFile) or overwrite:\n",
    "            # List all paths of songs\n",
    "            get_song_paths = glob.glob(inDir+'/*/*/*/*.h5')\n",
    "            \n",
    "            if not get_song_paths:\n",
    "                print \"No HDF5 (.h5) files foung in '%s'\"%(inDir)\n",
    "                print \"Check that the file structure under '%s' is /*/*/*/song_files.h5\"%(inDir)\n",
    "            else:\n",
    "                with open(outFile,'w') as f:\n",
    "                    f.writelines('\\n'.join(p for p in get_song_paths))\n",
    "                    f.close()\n",
    "                print \"File '%s' successfully created\"%(outFile)\n",
    "        else:\n",
    "            print \"File '%s' already exists\"%(outFile)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    Creates the file 'list_hdf5_files.txt' with the list of HDF5 files\n",
    "    \n",
    "    USE:\n",
    "    python list_MDS_files.py <path to songs> <save list path> <OPTIONAL overwrite>\n",
    "    \n",
    "    Paths should NOT include '/' at the end\n",
    "    If the file already exists, it will not be overwritten. Send 'True' to overwrite\n",
    "    '''\n",
    "    \n",
    "    input_path = sys.argv[1]  \n",
    "    output_path = sys.argv[2]\n",
    "    \n",
    "    # Option to overwrite current file\n",
    "    overwrite = False\n",
    "    if len(sys.argv) > 3:\n",
    "        overwrite  = sys.argv[3]\n",
    "    \n",
    "    main(input_path, output_path, overwrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scripts/list_LastFM_files.py\n",
    "\n",
    "**Script to create a list of JSON files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/list_LastFM_files.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/list_LastFM_files.py\n",
    "#!/usr/bin/env python\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "def main(inDir, outDir, overwrite = False):\n",
    "    \n",
    "    try:\n",
    "        os.path.exists(inDir)\n",
    "    except:\n",
    "        print \"Input file: '%s' does not exist\"%(inDir)\n",
    "    else:\n",
    "        outFile = outDir + '/list_lastfm_files.txt'\n",
    "        if not os.path.exists(outFile) or overwrite:\n",
    "            # List all paths of songs\n",
    "            get_song_paths = glob.glob(inDir+'/*/*/*/*.json')\n",
    "            \n",
    "            if not get_song_paths:\n",
    "                print \"No JSON files foung in '%s'\"%(inDir)\n",
    "                print \"Check that the file structure under '%s' is /*/*/*/song_files.json\"%(inDir)\n",
    "            else:\n",
    "                with open(outFile,'w') as f:\n",
    "                    f.writelines('\\n'.join(p for p in get_song_paths))\n",
    "                    f.close()\n",
    "                print  \"File '%s' successfully created\"%(outFile)\n",
    "        else:\n",
    "            print \"File '%s' already exists\"%(outFile)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    Creates the file 'list_lastfm_files.txt' with the list of HDF5 files\n",
    "    \n",
    "    USE:\n",
    "    python list_MDS_files.py <path to songs> <save list path> <OPTIONAL overwrite>\n",
    "    \n",
    "    Paths should NOT include '/' at the end\n",
    "    If the file already exists, it will not be overwritten. Send 'True' to overwrite\n",
    "    '''\n",
    "    input_path = sys.argv[1]  \n",
    "    output_path = sys.argv[2]\n",
    "    \n",
    "    # Option to overwrite current file\n",
    "    overwrite = False\n",
    "    if len(sys.argv) > 3:\n",
    "        overwrite  = sys.argv[3]\n",
    "    \n",
    "    main(input_path, output_path, overwrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test functions list_MDS_files.py and list_LastFM_files.pyt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'test/list_hdf5_files.txt' successfully created\r\n"
     ]
    }
   ],
   "source": [
    "!python scripts/list_MDS_files.py MillionSongSubset/data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'test/list_lastfm_files.txt' successfully created\r\n"
     ]
    }
   ],
   "source": [
    "!python scripts/list_LastFM_files.py MillionSongSubset/lastfm_subset test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1068\r\n",
      "-rw-rw-r-- 1 asoto asoto 509999 Dec 14 07:01 list_hdf5_files.txt\r\n",
      "-rw-rw-r-- 1 asoto asoto 578459 Dec 14 07:13 list_lastfm_files.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scripts/extractData.py \n",
    "\n",
    "**Script to process data and create part-00x files that have CSV format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/extractData.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/extractData.py\n",
    "#!/usr/bin/env python\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import shutil\n",
    "from pyspark import SparkContext\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "def parse_mismatches(line):\n",
    "    '''\n",
    "    This function extracts the songID and trackID of the mismatched records.\n",
    "    Returned value: ('songID', 'trackID')\n",
    "    '''\n",
    "    return line[8:45].split()\n",
    "\n",
    "\n",
    "def get_h5_info(path):\n",
    "    '''\n",
    "    Takes a path to a song stored as an HDF5 file and returns a dictionary with the \n",
    "    information that will be included in the graph\n",
    "    ''' \n",
    "    d = {}\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        song_id = f['metadata']['songs']['song_id'][0]\n",
    "        track_id = f['analysis']['songs']['track_id'][0]\n",
    "        \n",
    "        if [song_id, track_id] not in songsToRemove.value:\n",
    "\n",
    "            # --- Artist Info -----------------------------\n",
    "            d.setdefault('artist_id', f['metadata']['songs']['artist_id'][0])\n",
    "            d.setdefault('artist_mbid', f['metadata']['songs']['artist_mbid'][0])\n",
    "            d.setdefault('artist_7did', f['metadata']['songs']['artist_7digitalid'][0])\n",
    "            d.setdefault('artist_name', f['metadata']['songs']['artist_name'][0])\n",
    "\n",
    "            # --- Song Info -----------------------------\n",
    "            d.setdefault('song_id', song_id)\n",
    "            d.setdefault('track_id', track_id)\n",
    "            d.setdefault('title', f['metadata']['songs']['title'][0])\n",
    "            d.setdefault('dance', f['analysis']['songs']['danceability'][0])\n",
    "            d.setdefault('dur', f['analysis']['songs']['duration'][0])\n",
    "            d.setdefault('energy', f['analysis']['songs']['energy'][0])\n",
    "            d.setdefault('loudness', f['analysis']['songs']['loudness'][0])\n",
    "\n",
    "            # --- Year -----------------------------\n",
    "            d.setdefault('year', f['musicbrainz']['songs']['year'][0])\n",
    "\n",
    "            # --- Album -----------------------------\n",
    "            d.setdefault('album', f['metadata']['songs']['release'][0])\n",
    "\n",
    "            # --- Similar Artist -----------------------------\n",
    "            d.setdefault('a_similar', np.array(f['metadata']['similar_artists']))\n",
    "\n",
    "            # --- Artist Terms -----------------------------\n",
    "            d.setdefault('a_terms', np.array(f['metadata']['artist_terms']))\n",
    "            d.setdefault('a_tfrq', np.array(f['metadata']['artist_terms_freq']))\n",
    "            d.setdefault('a_tw', np.array(f['metadata']['artist_terms_weight']))\n",
    "\n",
    "            return d\n",
    "        else: \n",
    "            pass\n",
    "\n",
    "def get_json_info(path):\n",
    "    with open(path) as data_file:    \n",
    "        return json.load(data_file)\n",
    "\n",
    "def makeCSVline(line):\n",
    "    return ','.join(str(line[f]) for f in fieldsBrC.value)\n",
    "    \n",
    "def artistToTags(record):\n",
    "    '''\n",
    "    Concatenate artist with each tag\n",
    "    Normalize tag frequency and weight\n",
    "    '''\n",
    "    normalize_frq = record['a_tfrq'] / sum(record['a_tfrq'])\n",
    "    normalize_w = record['a_tw'] / sum(record['a_tw'])\n",
    "    terms = record['a_terms']\n",
    "    artist = record['artist_id']\n",
    "    \n",
    "    result = []\n",
    "    for i in range(len(terms)):\n",
    "        result.append( artist +\",\"+ terms[i] +\",\"+ str(normalize_frq[i]) +\",\"+ str(normalize_w[i]))\n",
    "    \n",
    "    return result\n",
    "\n",
    "def songToTags(record):\n",
    "    '''\n",
    "    Concatenate song with each tag\n",
    "    '''\n",
    "    tags = record['tags']\n",
    "    total_weight = sum(float(w[1]) for w in tags)\n",
    "    track_id = record['track_id']\n",
    "    \n",
    "    result = []\n",
    "    for i in range(len(tags)):\n",
    "        result.append( track_id +\",\"+ tags[i][0] +\",\"+ str(float(tags[i][1])/total_weight))\n",
    "    \n",
    "    return result\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    input_path: path to where the list of hdf5 and json files was created\n",
    "    output_path: a temporary directory where the Spark CSV files separated as part-000xx files will be stored\n",
    "    mismatch_path: path to where the mismatches file is located\n",
    "    \n",
    "    DO NOT INCLUDE '/' AT THE END OF PATH\n",
    "    Cannot change file names\n",
    "    '''\n",
    "    \n",
    "    inDir = sys.argv[1]  \n",
    "    outDir = sys.argv[2]\n",
    "    mismatch_path = sys.argv[3]\n",
    "    \n",
    "    #main(input_path, output_path)\n",
    "    # === Start Spark Context ===\n",
    "    sc = SparkContext(appName=\"SparkProcessing\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # === Load mismatches ===\n",
    "    toRemoveRDD = sc.textFile('file://'+mismatch_path+'/sid_mismatches.txt',32).map(parse_mismatches)\n",
    "    #songsToRemove = toRemoveRDD.collect()\n",
    "    songsToRemove = sc.broadcast(toRemoveRDD.collect())\n",
    "    \n",
    "    \n",
    "    # =====================================================================\n",
    "    # === Load list of files ====== Extract Song Data ===\n",
    "    song_pathsRDD   = sc.textFile('file://' + inDir + '/list_hdf5_files.txt',32)\n",
    "    lastfm_pathsRDD = sc.textFile('file://' + inDir + '/list_lastfm_files.txt',32)\n",
    "    \n",
    "    # === Extract Song Data ===\n",
    "    songsRDD = song_pathsRDD.map(get_h5_info).filter(lambda x: x<>None).cache()\n",
    "    \n",
    "    \n",
    "    # =====================================================================\n",
    "    # == Delete Sub-Folders ===\n",
    "    folders = ['/nodes_artists','/nodes_songs','/nodes_albums','/nodes_years','/nodes_tags',\n",
    "              '/rel_similar_artists', '/rel_performs','/rel_artist_has_album','/rel_artist_has_tag',\n",
    "               '/rel_song_in_album','/rel_similar_songs', '/rel_song_has_tag', '/rel_song_year']\n",
    "    for p in folders:\n",
    "        if os.path.exists(outDir+p):\n",
    "            shutil.rmtree(outDir+p)\n",
    "    \n",
    "    \n",
    "    # === ARTISTS ===\n",
    "    # CSV Format: artist_id, artist_mb_id, artist_7d_id, artist_name\n",
    "    fields = ['artist_id', 'artist_mbid', 'artist_7did', 'artist_name']\n",
    "    fieldsBrC = sc.broadcast(fields)\n",
    "    songsRDD.map(makeCSVline).distinct().saveAsTextFile('file://'+outDir+'/nodes_artists')\n",
    "    \n",
    "    # === SONGS ===\n",
    "    # CSV Format: song_id, track_id, song_title, danceability, duration, energy, loudness\n",
    "    fields = ['song_id', 'track_id', 'title', 'dance', 'dur', 'energy','loudness']\n",
    "    fieldsBrC = sc.broadcast(fields)\n",
    "    songsRDD.map(makeCSVline).distinct().saveAsTextFile('file://'+outDir+'/nodes_songs')\n",
    "    \n",
    "    # === ALBUMS ===\n",
    "    # CSV Format: album_name\n",
    "    songsRDD.map(lambda x: x['album']).distinct().saveAsTextFile('file://'+outDir+'/nodes_albums')\n",
    "    \n",
    "    # === YEAR ===\n",
    "    # CSV Format: year\n",
    "    songsRDD.map(lambda x: x['year']).filter(\n",
    "        lambda x: int(x) > 0).distinct().saveAsTextFile('file://'+outDir+'/nodes_years')\n",
    "\n",
    "    \n",
    "    # === SIMILAR_TO relationship between artist and artist ===\n",
    "    # CSV Format: from_artist_id, to_artist_id\n",
    "    # Similar Artist to Artist (directional, no properties)\n",
    "    similarArtistsRDD = songsRDD.map(lambda x: (x['artist_id'],x['a_similar'])).flatMapValues(lambda x: x)\n",
    "    similarArtistsRDD.distinct().map(lambda x: x[0]+\",\"+x[1]).saveAsTextFile('file://'+outDir+'/rel_similar_artists')\n",
    "    \n",
    "    # === PERFORMS relationship between artist and song ===\n",
    "    # CSV Format: artist_id, song_id\n",
    "    # Artist Performs Song (directional, no properties)\n",
    "    songsRDD.map(lambda x: x['artist_id']+\",\"+x['song_id']).distinct().saveAsTextFile('file://'+outDir+'/rel_performs')\n",
    "    \n",
    "    # === HAS_ALBUM relationship between artist and album  ===\n",
    "    # CSV Format: artist_id, album_name\n",
    "    # Artist Has Album (directional, no properties)\n",
    "    songsRDD.map(lambda x: x['artist_id']+\",\"+x['album']).distinct().saveAsTextFile('file://'+outDir+'/rel_artist_has_album')\n",
    "\n",
    "    # === HAS_TAG relationship between artist and tags ===\n",
    "    # CSV Format: artist_id, tag_name, tag_frequency, tag_weight\n",
    "    # Artist Has Tags (directional, has properties frequency and weight)\n",
    "    songsRDD.flatMap(artistToTags).distinct().saveAsTextFile('file://'+outDir+'/rel_artist_has_tag')\n",
    "    \n",
    "    # === RELEASED_ON relationship between song and year\n",
    "    # CSV Format: song_id, year\n",
    "    # Song Released in Year (directional, no properties)\n",
    "    songsRDD.filter(lambda x: int(x['year'])<>0).map(\n",
    "        lambda x: x['song_id']+\",\"+str(x['year'])).saveAsTextFile('file://'+outDir+'/rel_song_year')\n",
    "    \n",
    "    # === IN_ALBUM relationship between song and album ===\n",
    "    # CSV Format: song_id, album_name\n",
    "    # Song In Album (direction, no properties)   \n",
    "    songsRDD.map(lambda x: x['song_id']+\",\"+x['album']).distinct().saveAsTextFile('file://'+outDir+'/rel_song_in_album')\n",
    "    \n",
    "    # === Extract Lastfm Song Data ===\n",
    "    lastfmRDD = lastfm_pathsRDD.map(get_json_info).cache()\n",
    "    \n",
    "    # === TAGS ===\n",
    "    # CSV Format: tag_name\n",
    "    artistTags = songsRDD.flatMap(lambda x: x['a_terms']).distinct()\n",
    "    songTags = lastfmRDD.flatMap(lambda x: x['tags']).map(lambda x: x[0]).distinct()\n",
    "    allTags = songTags.union(artistTags).distinct()\n",
    "    allTags.saveAsTextFile('file://'+outDir+'/nodes_tags')\n",
    "    \n",
    "    # === SIMILAR_TO relationship between song and song ===\n",
    "    #CSV Format: from_track_id, to_track_id, similarity_measure\n",
    "    # Similar Song to Song (directional, with property similarity measure)   \n",
    "    similarSongsRDD = lastfmRDD.filter(\n",
    "        lambda x: x['similars']<>[]).map(\n",
    "        lambda x: (x['track_id'],x['similars'])).flatMapValues(lambda x: x)\n",
    "    similarSongsRDD.map(\n",
    "        lambda x: x[0]+\",\"+x[1][0]+\",\"+str(x[1][1])).saveAsTextFile('file://'+outDir+'/rel_similar_songs')\n",
    "    \n",
    "    # === HAS_TAG relationship between song and tags\n",
    "    # CSV Format: track_id, tag_name, tag_weight\n",
    "    lastfmRDD.flatMap(songToTags).saveAsTextFile('file://'+outDir+'/rel_song_has_tag')\n",
    "    \n",
    "    # === Stop Spark Context ===\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time /usr/bin/spark-submit --master local[32] \\\n",
    "/graph/W205_FinalProject/scripts/extractData.py /graph/import /graph/import/tmp /graph/import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1068\r\n",
      "-rw-rw-r-- 1 asoto asoto 509999 Dec 14 07:01 list_hdf5_files.txt\r\n",
      "-rw-rw-r-- 1 asoto asoto 578459 Dec 14 07:13 list_lastfm_files.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2336\r\n",
      "-rw-rw-r-- 1 asoto asoto   42666 Dec 14 07:53 DevCode.ipynb\r\n",
      "drwxr-xr-x 7 asoto asoto    4096 Dec 14 01:14 MillionSongSubset\r\n",
      "-rw-rw-r-- 1 asoto asoto  171467 Dec 14 07:53 Query_DontStopTheMusic.png\r\n",
      "-rw-rw-r-- 1 asoto asoto      36 Nov 29 17:24 README.md\r\n",
      "-rw-rw-r-- 1 asoto asoto    7429 Dec 14 09:45 Step 1 - Configuration.ipynb\r\n",
      "-rw-rw-r-- 1 asoto asoto   50014 Dec 14 09:45 Step 2 - Process Subset.ipynb\r\n",
      "-rw-rw-r-- 1 asoto asoto   23434 Dec 14 07:53 Step 3 - Load Subset to Neo.ipynb\r\n",
      "-rw-rw-r-- 1 asoto asoto   28074 Dec 14 15:37 Step 4 - Create General Scripts.ipynb\r\n",
      "drwxrwxr-x 2 asoto asoto    4096 Dec 14 08:52 config\r\n",
      "-rw-rw-r-- 1 asoto asoto     968 Nov 29 16:25 download_subsetdata.sh\r\n",
      "drwxr-xr-x 2 root  root     4096 Dec 14 08:55 images\r\n",
      "drwxrwxr-x 2 asoto asoto    4096 Dec 14 08:34 scripts\r\n",
      "-rw-rw-r-- 1 asoto asoto 2026182 Feb 12  2012 sid_mismatches.txt\r\n",
      "drwxrwxr-x 3 asoto asoto    4096 Dec 14 15:34 test\r\n",
      "drwxrwxr-x 2 asoto asoto    4096 Nov 29 07:02 test_code\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l /data/asoto/projectW205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x = [[u'SOUMNSI12AB0182807', u'TRMMGKQ128F9325E10'], [u'SOCMRBE12AB018C546', u'TRMMREB12903CEB1B1'], [u'SOLPHZY12AC468ABA8', u'TRMMBOC12903CEB46E'], [u'SONGHTM12A8C1374EF', u'TRMMITP128F425D8D0'], [u'SONGXCA12A8C13E82E', u'TRMMAYZ128F429ECE6'], [u'SOMBCRC12A67ADA435', u'TRMMNVU128EF343EED']]\n",
    "print [u'SOUMNSI12AB0182807', u'TRMMGKQ128F9325E10'] in x\n",
    "print [u'SOUMNSI12AB0182807', u'TRMMGKQ128F9325E10'] not in x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd /graph\n",
    "for dirname in `ls import/tmp/`\n",
    "do\n",
    "for j in `ls import/tmp/$dirname/part*`\n",
    "do echo $j\n",
    "done\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import shutil\n",
    "from pyspark import SparkContext\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "def parse_mismatches(line):\n",
    "    '''\n",
    "    This function extracts the songID and trackID of the mismatched records.\n",
    "    Returned value: ('songID', 'trackID')\n",
    "    '''\n",
    "    return line[8:45].split()\n",
    "\n",
    "\n",
    "def get_h5_info(path):\n",
    "    '''\n",
    "    Takes a path to a song stored as an HDF5 file and returns a dictionary with the \n",
    "    information that will be included in the graph\n",
    "    '''\n",
    "    d = {}\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        song_id = f['metadata']['songs']['song_id'][0]\n",
    "        track_id = f['analysis']['songs']['track_id'][0]\n",
    "\n",
    "        if [song_id, track_id] not in songsToRemove.value:\n",
    "\n",
    "            # --- Artist Info -----------------------------\n",
    "            d.setdefault('artist_id', f['metadata']['songs']['artist_id'][0])\n",
    "            d.setdefault('artist_mbid', f['metadata']['songs']['artist_mbid'][0])\n",
    "            d.setdefault('artist_7did', f['metadata']['songs']['artist_7digitalid'][0])\n",
    "            d.setdefault('artist_name', f['metadata']['songs']['artist_name'][0])\n",
    "\n",
    "            # --- Song Info -----------------------------\n",
    "            d.setdefault('song_id', song_id)\n",
    "            d.setdefault('track_id', track_id)\n",
    "            d.setdefault('title', f['metadata']['songs']['title'][0])\n",
    "            d.setdefault('dance', f['analysis']['songs']['danceability'][0])\n",
    "            d.setdefault('dur', f['analysis']['songs']['duration'][0])\n",
    "            d.setdefault('energy', f['analysis']['songs']['energy'][0])\n",
    "            d.setdefault('loudness', f['analysis']['songs']['loudness'][0])\n",
    "\n",
    "            # --- Year -----------------------------\n",
    "            d.setdefault('year', f['musicbrainz']['songs']['year'][0])\n",
    "\n",
    "            # --- Album -----------------------------\n",
    "            d.setdefault('album', f['metadata']['songs']['release'][0])\n",
    "\n",
    "            # --- Similar Artist -----------------------------\n",
    "            d.setdefault('a_similar', np.array(f['metadata']['similar_artists']))\n",
    "\n",
    "            # --- Artist Terms -----------------------------\n",
    "            d.setdefault('a_terms', np.array(f['metadata']['artist_terms']))\n",
    "            d.setdefault('a_tfrq', np.array(f['metadata']['artist_terms_freq']))\n",
    "            d.setdefault('a_tw', np.array(f['metadata']['artist_terms_weight']))\n",
    "\n",
    "            return d\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "def get_json_info(path):\n",
    "    with open(path) as data_file:\n",
    "        return json.load(data_file)\n",
    "\n",
    "def makeCSVline(line):\n",
    "    return ','.join(str(line[f]) for f in fieldsBrC.value)\n",
    "\n",
    "def artistToTags(record):\n",
    "    '''\n",
    "    Concatenate artist with each tag\n",
    "    Normalize tag frequency and weight\n",
    "    '''\n",
    "    normalize_frq = record['a_tfrq'] / sum(record['a_tfrq'])\n",
    "    normalize_w = record['a_tw'] / sum(record['a_tw'])\n",
    "    terms = record['a_terms']\n",
    "    artist = record['artist_id']\n",
    "\n",
    "    result = []\n",
    "    for i in range(len(terms)):\n",
    "        result.append( artist +\",\"+ terms[i] +\",\"+ str(normalize_frq[i]) +\",\"+ str(normalize_w[i]))\n",
    "\n",
    "    return result\n",
    "\n",
    "def songToTags(record):\n",
    "    '''\n",
    "    Concatenate song with each tag\n",
    "    '''\n",
    "    tags = record['tags']\n",
    "    total_weight = sum(float(w[1]) for w in tags)\n",
    "    track_id = record['track_id']\n",
    "\n",
    "    result = []\n",
    "    for i in range(len(tags)):\n",
    "        result.append( track_id +\",\"+ tags[i][0] +\",\"+ str(float(tags[i][1])/total_weight))\n",
    "\n",
    "    return result\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    input_path: path to where the list of hdf5 and json files was created\n",
    "    output_path: a temporary directory where the Spark CSV files separated as part-000xx files will be stored\n",
    "    mismatch_path: path to where the mismatches file is located\n",
    "    \n",
    "    DO NOT INCLUDE '/' AT THE END OF PATH\n",
    "    Cannot change file names\n",
    "    '''\n",
    "    inDir = sys.argv[1]\n",
    "    outDir = sys.argv[2]\n",
    "    mismatch_path = sys.argv[3]\n",
    "\n",
    "    #main(input_path, output_path)\n",
    "    # === Start Spark Context ===\n",
    "    sc = SparkContext(appName=\"SparkProcessing\")\n",
    "\n",
    "    # =====================================================================\n",
    "    # === Load mismatches ===\n",
    "    toRemoveRDD = sc.textFile('file://'+mismatch_path+'/sid_mismatches.txt',32).map(parse_mismatches)\n",
    "    #songsToRemove = toRemoveRDD.collect()\n",
    "    songsToRemove = sc.broadcast(toRemoveRDD.collect())\n",
    "\n",
    "\n",
    "    # =====================================================================\n",
    "    # === Load list of files ===\n",
    "    song_pathsRDD   = sc.textFile('file://' + inDir + '/list_hdf5_files.txt',32)\n",
    "    lastfm_pathsRDD = sc.textFile('file://' + inDir + '/list_lastfm_files.txt',32)\n",
    "\n",
    "    # === Extract Song Data ===\n",
    "    songsRDD = song_pathsRDD.map(get_h5_info).filter(lambda x: x<>None).cache()\n",
    "    #lastfmRDD = lastfm_pathsRDD.map(get_json_info).cache()\n",
    "\n",
    "\n",
    "    # =====================================================================\n",
    "    # == Delete Sub-Folders ===\n",
    "    folders = ['/nodes_artists','/nodes_songs','/nodes_albums','/nodes_years','/nodes_tags',\n",
    "              '/rel_similar_artists', '/rel_performs','/rel_artist_has_album','/rel_artist_has_tag',\n",
    "               '/rel_song_in_album','/rel_similar_songs', '/rel_song_has_tag', '/rel_song_year']\n",
    "    for p in folders:\n",
    "        if os.path.exists(outDir+p):\n",
    "            shutil.rmtree(outDir+p)\n",
    "\n",
    "    # =====================================================================\n",
    "    # === Create Nodes ===\n",
    "\n",
    "    # === ARTISTS ===\n",
    "    # CSV Format: artist_id, artist_mb_id, artist_7d_id, artist_name\n",
    "    fields = ['artist_id', 'artist_mbid', 'artist_7did', 'artist_name']\n",
    "    fieldsBrC = sc.broadcast(fields)\n",
    "    songsRDD.map(makeCSVline).distinct().saveAsTextFile('file://'+outDir+'/nodes_artists')\n",
    "\n",
    "    # === SONGS ===\n",
    "    # CSV Format: song_id, track_id, song_title, danceability, duration, energy, loudness\n",
    "    fields = ['song_id', 'track_id', 'title', 'dance', 'dur', 'energy','loudness']\n",
    "    fieldsBrC = sc.broadcast(fields)\n",
    "    songsRDD.map(makeCSVline).distinct().saveAsTextFile('file://'+outDir+'/nodes_songs')\n",
    "\n",
    "    # === ALBUMS ===\n",
    "    # CSV Format: album_name\n",
    "    songsRDD.map(lambda x: x['album']).distinct().saveAsTextFile('file://'+outDir+'/nodes_albums')\n",
    "\n",
    "    # === YEAR ===\n",
    "    # CSV Format: year\n",
    "    songsRDD.map(lambda x: x['year']).filter(\n",
    "        lambda x: int(x) > 0).distinct().saveAsTextFile('file://'+outDir+'/nodes_years')\n",
    "\n",
    "    # === TAGS ===\n",
    "    # CSV Format: tag_name\n",
    "    #artistTags = songsRDD.flatMap(lambda x: x['a_terms']).distinct()\n",
    "    #songTags = lastfmRDD.flatMap(lambda x: x['tags']).map(lambda x: x[0]).distinct()\n",
    "    #allTags = songTags.union(artistTags).distinct()\n",
    "    #allTags.saveAsTextFile('file://'+outDir+'/nodes_tags')\n",
    "\n",
    "    # =====================================================================\n",
    "    # === Create Relationships ===\n",
    "\n",
    "    # === SIMILAR_TO relationship between artist and artist ===\n",
    "    # CSV Format: from_artist_id, to_artist_id\n",
    "    # Similar Artist to Artist (directional, no properties)\n",
    "    similarArtistsRDD = songsRDD.map(lambda x: (x['artist_id'],x['a_similar'])).flatMapValues(lambda x: x)\n",
    "    similarArtistsRDD.distinct().map(lambda x: x[0]+\",\"+x[1]).saveAsTextFile('file://'+outDir+'/rel_similar_artists')\n",
    "\n",
    "    # === PERFORMS relationship between artist and song ===\n",
    "    # CSV Format: artist_id, song_id\n",
    "    # Artist Performs Song (directional, no properties)\n",
    "    songsRDD.map(lambda x: x['artist_id']+\",\"+x['song_id']).distinct().saveAsTextFile('file://'+outDir+'/rel_performs')\n",
    "\n",
    "    # === HAS_ALBUM relationship between artist and album  ===\n",
    "    # CSV Format: artist_id, album_name\n",
    "    # Artist Has Album (directional, no properties)\n",
    "    songsRDD.map(lambda x: x['artist_id']+\",\"+x['album']).distinct().saveAsTextFile('file://'+outDir+'/rel_artist_has_album')\n",
    "\n",
    "    # === HAS_TAG relationship between artist and tags ===\n",
    "    # CSV Format: artist_id, tag_name, tag_frequency, tag_weight\n",
    "    # Artist Has Tags (directional, has properties frequency and weight)\n",
    "    songsRDD.flatMap(artistToTags).distinct().saveAsTextFile('file://'+outDir+'/rel_artist_has_tag')\n",
    "\n",
    "    # === IN_ALBUM relationship between song and album ===\n",
    "    # CSV Format: song_id, album_name\n",
    "    # Song In Album (direction, no properties)   \n",
    "    songsRDD.map(lambda x: x['song_id']+\",\"+x['album']).distinct().saveAsTextFile('file://'+outDir+'/rel_song_in_album')\n",
    "\n",
    "    # === RELEASED_ON relationship between song and year\n",
    "    # CSV Format: song_id, year\n",
    "    # Song Released in Year (directional, no properties)\n",
    "    songsRDD.filter(lambda x: int(x['year'])<>0).map(\n",
    "        lambda x: x['song_id']+\",\"+str(x['year'])).saveAsTextFile('file://'+outDir+'/rel_song_year')\n",
    "\n",
    "    lastfmRDD = lastfm_pathsRDD.map(get_json_info).cache()\n",
    "\n",
    "    # === TAGS ===\n",
    "    # CSV Format: tag_name\n",
    "    artistTags = songsRDD.flatMap(lambda x: x['a_terms']).distinct()\n",
    "    songTags = lastfmRDD.flatMap(lambda x: x['tags']).map(lambda x: x[0]).distinct()\n",
    "    allTags = songTags.union(artistTags).distinct()\n",
    "                                                          \n",
    "    songsRDD.uncache()\n",
    "    # === SIMILAR_TO relationship between song and song ===\n",
    "    #CSV Format: from_track_id, to_track_id, similarity_measure\n",
    "    # Similar Song to Song (directional, with property similarity measure)   \n",
    "    similarSongsRDD = lastfmRDD.filter(\n",
    "        lambda x: x['similars']<>[]).map(\n",
    "        lambda x: (x['track_id'],x['similars'])).flatMapValues(lambda x: x)\n",
    "    similarSongsRDD.map(\n",
    "        lambda x: x[0]+\",\"+x[1][0]+\",\"+str(x[1][1])).saveAsTextFile('file://'+outDir+'/rel_similar_songs')\n",
    "\n",
    "    # === HAS_TAG relationship between song and tags\n",
    "    # CSV Format: track_id, tag_name, tag_weight\n",
    "    lastfmRDD.flatMap(songToTags).saveAsTextFile('file://'+outDir+'/rel_song_has_tag')\n",
    "\n",
    "    # === RELEASED_ON relationship between song and year\n",
    "    # CSV Format: song_id, year\n",
    "    # Song Released in Year (directional, no properties)\n",
    "    #songsRDD.filter(lambda x: int(x['year'])<>0).map(\n",
    "    #    lambda x: x['song_id']+\",\"+str(x['year'])).saveAsTextFile('file://'+outDir+'/rel_song_year')\n",
    "\n",
    "\n",
    "\n",
    "    # === Stop Spark Context ===\n",
    "    sc.stop()\n",
    "                                                                                                                                                                                           243,5         Bot\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
